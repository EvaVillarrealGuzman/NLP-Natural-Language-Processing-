{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpFfwpFuoBPj"
   },
   "source": [
    "# Chapter 2: Large scale data analysis with Spacy\n",
    "\n",
    "- https://course.spacy.io/chapter2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2JDWrBYpRXH"
   },
   "source": [
    "# 1)- Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cODT6eurhVF"
   },
   "outputs": [],
   "source": [
    "# import key modules related to text data\n",
    "import spacy\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jB7-ddzor-GL"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pX2SZQ4AooEL",
    "outputId": "72be62a2-094f-42b5-fdfe-ada8e26e8f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value:', nlp.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_wJIYgTmooHl",
    "outputId": "bf607a18-b28a-4fe5-9701-44516841668a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string value: coffee\n"
     ]
    }
   ],
   "source": [
    "print('string value:', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5WXEZHNtwUF"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7GckMWZkooKk",
    "outputId": "c731e79a-31d7-4f10-c933-b7b90cd76354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SG8BExeGuRdC"
   },
   "source": [
    "- Word text: lexeme.text and lexeme.orth (the hash)\n",
    "- Lexical attributes like lexeme.is_alpha\n",
    "- Not context-dependent part-of-speech tags, dependencies or entity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7NibsC1wRg6"
   },
   "source": [
    "### Part-1:Strings to hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yYlo0T2XooNx",
    "outputId": "502691ce-3d72-4e0b-8867-19f7ab346555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5439657043933447811\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings['cat']\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string =nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlONkLUzysaY"
   },
   "source": [
    "### Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "abqudDFUooZ6",
    "outputId": "cc1c2485-7293-4e10-a7b9-9a25640ba470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "# Look up the hash for the string label \"PERSON\"\n",
    "person_hash = nlp.vocab.strings['PERSON']\n",
    "print(person_hash)\n",
    "\n",
    "# Look up the person_hash to get the string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQgUA6uM2tug"
   },
   "source": [
    "# 2)- Data Structures (2) : Doc , span and Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEDjl9jZx6QG"
   },
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [False, True, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "olo8Fsipx6ZW",
    "outputId": "55fd10a0-7f81-4a3f-8003-c3af8c20ec6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloworld !\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3nNCiG04IjZ"
   },
   "source": [
    "### Span Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QtZeueOQx6h4",
    "outputId": "bd3be539-f2c4-4e6c-c45e-40ecd8e19508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m-7K_zdvx6tW",
    "outputId": "e6de3c7a-f3e2-42a1-8c5b-4ab48d63f6bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YmJQs0Nqx61A",
    "outputId": "1cd12a34-1598-4a61-e616-2b7b48551450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "print(span_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WbPCAPjd5Kc0",
    "outputId": "65a3df7a-f34d-4e4e-dbcf-dd182fcdf8a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "print(span_with_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRp6H7Xc7SMi"
   },
   "source": [
    "### Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VSORdq8y5Uyj",
    "outputId": "20daa64a-57a8-4016-849d-e5ce611ddeb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEVv38ZJ85N9"
   },
   "source": [
    "### Part 2:\n",
    "\n",
    "- Import the Doc from spacy.tokens.\n",
    "- Create a Doc from the words and spaces. Don’t forget to pass in the vocab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VWGe3ug47VR8",
    "outputId": "cf88d335-b32d-4596-a246-6cd357543e71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMZ1PRMD9upA"
   },
   "source": [
    "### Part 3\n",
    "\n",
    "- Import the Doc from spacy.tokens.\n",
    "- Complete the words and spaces to match the desired text and create a doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I_c7JF0A9Gj0",
    "outputId": "f3bce92c-94c2-49ae-ef40-711b457eeac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBHwjTZt_Q-d"
   },
   "source": [
    "# 3)- Docs, spans and entities from scratch\n",
    "\n",
    "\n",
    "- Use the Doc class directly to create a doc from the words and spaces.\n",
    "- Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\".\n",
    "- Overwrite the doc.ents with a list of one entity, the “David Bowie” span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "wa4xsyI4904y",
    "outputId": "ecb8763b-19f5-4aac-d8d9-24f9f269dd56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q26AywLSDdMS"
   },
   "source": [
    "# 4)- Get specific POS from text\n",
    "\n",
    "- Write code so as to analyze a text and collect all proper nouns that are followed by a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iUqcUsn8_rXO",
    "outputId": "3194d875-5738-46fe-f041-c674b4862030"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin is a nice city\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Check if the current token is a proper noun\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Check if the next token is a verb\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Found proper noun before a verb:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mQE97eDND9Bi"
   },
   "source": [
    "# 5)- Word vectors and semantic similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c85sMs8nJTyI"
   },
   "source": [
    "### Inspecting word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "PSvtOtRf_rb-",
    "outputId": "d92630f0-2ae3-413e-d524-1c096ada9d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2387033   0.6563978  -1.3915625  -2.0158877  -2.6757379   0.6049806\n",
      " -4.9210434   2.6724901  -0.5421836  -0.8809319   2.6590538  -3.5114794\n",
      "  5.2498507  -1.2459521   0.87745774 -1.4979573  -2.1457722  -1.1067222\n",
      " -2.1124492  -1.3459014   1.1430457  -0.8469293   3.5861008   2.6726484\n",
      " -2.3294182   2.974951   -0.02186409 -0.6708566   0.02608067  4.3714023\n",
      " -5.7176323  -0.4561702   1.0577139  -2.1087644  -1.9045851   2.221695\n",
      "  0.6667701   2.1882637  -2.8947964   3.4675171   1.1178943   1.1830177\n",
      " -0.02594751  1.6294907   5.041691    0.4304657   0.04791719  0.5513984\n",
      " -0.36579067 -3.9674122   0.625882   -1.8514493   2.196616    2.5510483\n",
      "  5.348059   -0.4365843   0.2364122  -0.630581   -1.3520204  -0.53069663\n",
      " -1.686786   -2.9399247   3.8511808   1.1052506  -2.691662    0.39372563\n",
      "  0.34798118 -4.6369677   2.3882837   0.8316821  -1.1286397  -4.356789\n",
      " -1.0592571  -1.6973286  -0.05940416 -1.4840398  -0.2224785   1.9230247\n",
      " -0.56688464  2.5266325  -1.8940659  -0.01945698 -0.8910403   3.5121229\n",
      "  1.4605861   0.27346015 -0.53729135  2.5454154   4.870879   -1.7280082\n",
      " -2.4584932  -1.2271122   2.809289   -0.4340926  -0.36866692  0.5186993 ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the en_core_web_md model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"Two bananas in pyjamas\")\n",
    "\n",
    "# Get the vector for the token \"bananas\"\n",
    "bananas_vector = doc[1].vector\n",
    "print(bananas_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYLK7NP-PSFT"
   },
   "source": [
    "# 6)- Comparing similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSO3UmjWQV4P"
   },
   "source": [
    "### Part 1:\n",
    "\n",
    "- Use the doc.similarity method to compare doc1 to doc2 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "gi1WIKeFR3UX",
    "outputId": "62d6356f-880f-4566-feb2-df850c35221c"
   },
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXuZYfn6_rhb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8789265574516525\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"It's a warm summer day\")\n",
    "doc2 = nlp(\"It's sunny outside\")\n",
    "\n",
    "# Get the similarity of doc1 and doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZgk9BPrRLEv"
   },
   "source": [
    "### Part 2\n",
    "\n",
    "- Use the token.similarity method to compare token1 to token2 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzI5xsqUPbZ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2232533\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"TV and books\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Get the similarity of the tokens \"TV\" and \"books\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAUODA8-S8tm"
   },
   "source": [
    "### Part 3\n",
    "\n",
    "- Create spans for “great restaurant”/“really nice bar”.\n",
    "- Use span.similarity to compare them and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VFrThLoPbfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7517392\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# Create spans for \"great restaurant\" and \"really nice bar\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# Get the similarity of the spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tQm7cy8PblW"
   },
   "source": [
    "# 7)-Combining models and rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Po-sTj3pPbrF"
   },
   "source": [
    "### a)- Rule based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9cngWVdPbw4"
   },
   "outputs": [],
   "source": [
    "# Initialize with the shared vocab\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Patterns are lists of dictionaries describing the tokens\n",
    "pattern = [{'LEMMA': 'love', 'POS': 'VERB'}, {'LOWER': 'cats'}]\n",
    "matcher.add('LOVE_CATS', None, pattern)\n",
    "\n",
    "# Operators can specify how often a token should be matched\n",
    "pattern = [{'TEXT': 'very', 'OP': '+'}, {'TEXT': 'happy'}]\n",
    "matcher.add('VERY_HAPPY', None, pattern)\n",
    "\n",
    "# Calling matcher on doc returns list of (match_id, start, end) tuples\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCe4yZV0Pb22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9137535031263442622, 1, 3),\n",
       " (2447047934687575526, 7, 9),\n",
       " (2447047934687575526, 6, 9)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoDMHtgoPb-p"
   },
   "source": [
    "### b)-statistical predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vri0xa6nPb9L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n",
      "Root token: Retriever\n",
      "Root head token: have\n",
      "Previous token: a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRakwi_tPbU9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'determiner'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"DET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbK90dD9907i"
   },
   "source": [
    "### c)-Efficient phrase matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWQ5oTUJ90-y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)- Extract key words from text\n",
    "\n",
    "- Amazon\n",
    "\n",
    "- ad-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
