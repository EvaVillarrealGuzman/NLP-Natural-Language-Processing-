{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpFfwpFuoBPj"
   },
   "source": [
    "# Chapter 4: Training a neural network model\n",
    "\n",
    "- https://course.spacy.io/chapter4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How training works \n",
    "\n",
    "- Initialize the model weights randomly with nlp.begin_training\n",
    "- Predict a few examples with the current weights by calling nlp.update\n",
    "- Compare prediction with true labels\n",
    "- Calculate how to change weights to improve predictions\n",
    "- Update weights slightly\n",
    "- Go back to 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)-Creating training data\n",
    "\n",
    "spaCy’s rule-based Matcher is a great way to quickly create training data for named entity models. A list of sentences is available as the variable TEXTS. You can print it the IPython shell to inspect it. We want to find all mentions of different iPhone models, so we can create training data to teach a model to recognize them as 'GADGET'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to preorder the iPhone X',\n",
       " 'iPhone X is coming',\n",
       " 'Should I pay $1,000 for the iPhone X?',\n",
       " 'The iPhone 8 reviews are here',\n",
       " 'Your iPhone goes up to 11 today',\n",
       " 'I need a new phone! Any tips?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "\n",
    "# Add patterns to the matcher\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.matcher.matcher.Matcher object at 0x12251f0e0>\n"
     ]
    }
   ],
   "source": [
    "print(matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TEXTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=str(TEXTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['How to preorder the iPhone X', 'iPhone X is coming', 'Should I pay $1,000 for the iPhone X?', 'The iPhone 8 reviews are here', 'Your iPhone goes up to 11 today', 'I need a new phone! Any tips?']\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to preorder the iPhone X', 'iPhone X is coming', 'Should I pay $1,000 for the iPhone X?', 'The iPhone 8 reviews are here', 'Your iPhone goes up to 11 today', 'I need a new phone! Any tips?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_2=nlp(doc)\n",
    "doc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', \"'\", 'How', 'to', 'preorder', 'the', 'iPhone', 'X', \"'\", ',', \"'\", 'iPhone', 'X', 'is', 'coming', \"'\", ',', \"'\", 'Should', 'I', 'pay', '$', '1,000', 'for', 'the', 'iPhone', 'X', '?', \"'\", ',', \"'\", 'The', 'iPhone', '8', 'reviews', 'are', 'here', \"'\", ',', \"'\", 'Your', 'iPhone', 'goes', 'up', 'to', '11', 'today', \"'\", ',', \"'\", 'I', 'need', 'a', 'new', 'phone', '!', 'Any', 'tips', '?', \"'\", ']']\n"
     ]
    }
   ],
   "source": [
    "print([t.text for t in doc_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(274229460355380281, 6, 8),\n",
       " (274229460355380281, 6, 7),\n",
       " (274229460355380281, 11, 13),\n",
       " (274229460355380281, 11, 12),\n",
       " (274229460355380281, 25, 27),\n",
       " (274229460355380281, 25, 26),\n",
       " (274229460355380281, 32, 33),\n",
       " (274229460355380281, 32, 34),\n",
       " (274229460355380281, 41, 42)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc_2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n",
      "iPhone\n",
      "iPhone X\n",
      "iPhone\n",
      "iPhone X\n",
      "iPhone\n",
      "iPhone\n",
      "iPhone 8\n",
      "iPhone\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc_2[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2:\n",
    "    \n",
    "Let’s use the match patterns we’ve created in the previous exercise to bootstrap a set of training examples. A list of sentences is available as the variable TEXTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"iphone.json\") as f:\n",
    "    TEXTS = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to preorder the iPhone X',\n",
       " 'iPhone X is coming',\n",
       " 'Should I pay $1,000 for the iPhone X?',\n",
       " 'The iPhone 8 reviews are here',\n",
       " 'Your iPhone goes up to 11 today',\n",
       " 'I need a new phone! Any tips?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True, \"OP\": \"?\"}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 10, 'GADGET'), (4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "\n",
    "print(*TRAINING_DATA, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)-Training Loop\n",
    "\n",
    "- The steps of a training loop\n",
    "- Loop for a number of times.\n",
    "- Shuffle the training data.\n",
    "- Divide the data into batches.\n",
    "- Update the model for each batch.\n",
    "- Save the updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    # Create batches and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting new pipeline from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with blank English model\n",
    "nlp = spacy.blank('en')\n",
    "# Create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner)\n",
    "# Add a new label\n",
    "ner.add_label('GADGET')\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "# Train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    # Divide examples into batches\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)-Setting up the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label(\"GADGET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)-Building a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "with open(\"gadgets.json\") as f:\n",
    "    TRAINING_DATA = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['How to preorder the iPhone X', {'entities': [[20, 28, 'GADGET']]}],\n",
       " ['iPhone X is coming', {'entities': [[0, 8, 'GADGET']]}],\n",
       " ['Should I pay $1,000 for the iPhone X?', {'entities': [[28, 36, 'GADGET']]}],\n",
       " ['The iPhone 8 reviews are here', {'entities': [[4, 12, 'GADGET']]}],\n",
       " ['Your iPhone goes up to 11 today', {'entities': [[5, 11, 'GADGET']]}],\n",
       " ['I need a new phone! Any tips?', {'entities': []}]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"GADGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 10.833333492279053}\n",
      "{'ner': 21.11037278175354}\n",
      "{'ner': 32.944974422454834}\n",
      "{'ner': 10.338136613368988}\n",
      "{'ner': 17.314597129821777}\n",
      "{'ner': 21.89810425043106}\n",
      "{'ner': 1.8344886228442192}\n",
      "{'ner': 4.833294221898541}\n",
      "{'ner': 8.11140689929016}\n",
      "{'ner': 1.1629105684405658}\n",
      "{'ner': 3.208279492217116}\n",
      "{'ner': 5.001780176447937}\n",
      "{'ner': 4.669799611903727}\n",
      "{'ner': 6.814525853376836}\n",
      "{'ner': 10.169673154130578}\n",
      "{'ner': 2.9788101445883512}\n",
      "{'ner': 3.9745154915872263}\n",
      "{'ner': 6.346071177118574}\n",
      "{'ner': 1.0001555554335937}\n",
      "{'ner': 2.775552038263413}\n",
      "{'ner': 3.6978900395915844}\n",
      "{'ner': 1.4259431460814085}\n",
      "{'ner': 1.599497755166908}\n",
      "{'ner': 1.725024571223571}\n",
      "{'ner': 0.7875489346955078}\n",
      "{'ner': 0.7933864060581186}\n",
      "{'ner': 0.7936531797922908}\n",
      "{'ner': 0.0006064233625888704}\n",
      "{'ner': 2.180438003223818}\n",
      "{'ner': 2.1806718718755658}\n"
     ]
    }
   ],
   "source": [
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\n",
    "numbers printed to the shell represent the loss on each iteration, the amount of\n",
    "work left for the optimizer. The lower the number, the better. In real life, you\n",
    "normally want to use *a lot* more data than this, ideally at least a few hundred\n",
    "or a few thousand examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)-Training multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\"PewDiePie smashes YouTube record\", {\"entities\": [(18, 25, \"WEBSITE\")]}),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\")]},\n",
    "    ),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its case of one class labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.\n",
    "\n",
    "Update the training data to include annotations for the PERSON entities “PewDiePie” and “Alexis Ohanian”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [\n",
    "    (\n",
    "        \"Reddit partners with Patreon to help creators build communities\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (21, 28, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"PewDiePie smashes YouTube record\",\n",
    "        {\"entities\": [(0, 9, \"PERSON\"), (18, 25, \"WEBSITE\")]},\n",
    "    ),\n",
    "    (\n",
    "        \"Reddit founder Alexis Ohanian gave away two Metallica tickets to fans\",\n",
    "        {\"entities\": [(0, 6, \"WEBSITE\"), (15, 29, \"PERSON\")]},\n",
    "    ),\n",
    "    # And so on...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other advanced Annotation tools such as Brat and Prodigy. Manual labeling is long and tiresome work."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
