{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5.Translation_attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z31QoFe9tWwt"
      },
      "source": [
        "# Sequence-to-Sequence Model with Attention\n",
        "\n",
        "- Seq2seq models are used for applications such as machine translation and image caption generation.We shall apply machine translate as an example.\n",
        "\n",
        "- We will build a seq2seq model with attention in PyTorch for translating English to French.\n",
        "\n",
        "- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)]\n",
        "\n",
        "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/): A great blog post to learn seq2seq model with attention wth visual explanations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfLpUMQsYVoi",
        "colab_type": "text"
      },
      "source": [
        "# 1)- Import key Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yC9rlOUlkLd3",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from nltk import wordpunct_tokenize\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from tqdm import tqdm_notebook, tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTkgu_5Dt2q4"
      },
      "source": [
        "In order to perform deep learning on a GPU (so that everything runs super quick!), CUDA has to be installed and configured. Fortunately, Google Colab already has this set up, but if you want to try this on your own GPU, you can [install CUDA from here](https://developer.nvidia.com/cuda-downloads). Make sure you also [install cuDNN](https://developer.nvidia.com/cudnn) for optimized performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qYu99uVmkQ90",
        "outputId": "597bd808-6e40-483a-8880-2ab8a080e99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gy8_AvfmlQXV"
      },
      "source": [
        "# 2)- Load the data\n",
        "\n",
        "We will download a dataset of English-to-French translations from a public Google Drive folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTgV3JpYX5L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = 'data/english_to_french.txt'\n",
        "if not Path(DATA_PATH).is_file():\n",
        "    gdd.download_file_from_google_drive(\n",
        "        file_id='1Jf7QoW2NK6_ayEXZji6DAXDSIRMvapm3',\n",
        "        dest_path=DATA_PATH,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWbkoV_YAUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "f78c99f7-d492-4c5d-879c-033ece413041"
      },
      "source": [
        "# View some example records\n",
        "df=pd.read_table(DATA_PATH, names=['english','french'])\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>French</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Va !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Cours !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!</td>\n",
              "      <td>Courez !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>Ça alors !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fire!</td>\n",
              "      <td>Au feu !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  English      French\n",
              "0     Go.        Va !\n",
              "1    Run!     Cours !\n",
              "2    Run!    Courez !\n",
              "3    Wow!  Ça alors !\n",
              "4   Fire!    Au feu !"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvVziQM3YSYd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b057d2c3-8371-427f-fc60-3b7cce9afa56"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145437, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psuDEEKnYax6",
        "colab_type": "text"
      },
      "source": [
        "# 3)- Pre-process & clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g06tIZtnkV4J",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"Turn text into discrete tokens.\n",
        "\n",
        "    Remove tokens that are not words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = wordpunct_tokenize(text)\n",
        "\n",
        "    # Only keep words\n",
        "    tokens = [token for token in tokens\n",
        "              if all(char.isalpha() for char in token)]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class EnglishFrenchTranslations(Dataset):\n",
        "    def __init__(self, path, max_vocab):\n",
        "        self.max_vocab = max_vocab\n",
        "        \n",
        "        # Extra tokens to add\n",
        "        self.padding_token = '<PAD>'\n",
        "        self.start_of_sequence_token = '<SOS>'\n",
        "        self.end_of_sequence_token = '<EOS>'\n",
        "        self.unknown_word_token = '<UNK>'\n",
        "        \n",
        "        # Helper function\n",
        "        self.flatten = lambda x: [sublst for lst in x for sublst in lst]\n",
        "        \n",
        "        # Load the data into a DataFrame\n",
        "        df = pd.read_csv(path, names=['english', 'french'], sep='\\t')\n",
        "        \n",
        "        # Tokenize inputs (English) and targets (French)\n",
        "        self.tokenize_df(df)\n",
        "\n",
        "        # To reduce computational complexity, replace rare words with <UNK>i.e out of vocan concept\n",
        "        self.replace_rare_tokens(df)\n",
        "        \n",
        "        # Prepare variables with mappings of tokens to indices\n",
        "        self.create_token2idx(df)\n",
        "        \n",
        "        # Remove sequences with mostly <UNK>\n",
        "        df = self.remove_mostly_unk(df)\n",
        "        \n",
        "        # Every sequence (input and target) should start with <SOS>\n",
        "        # and end with <EOS>\n",
        "        self.add_start_and_end_to_tokens(df)\n",
        "        \n",
        "        # Convert tokens to indices\n",
        "        self.tokens_to_indices(df)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Return example at index idx.\"\"\"\n",
        "        return self.indices_pairs[idx][0], self.indices_pairs[idx][1]\n",
        "    \n",
        "    def tokenize_df(self, df):\n",
        "        \"\"\"Turn inputs and targets into tokens.\"\"\"\n",
        "        df['tokens_inputs'] = df.english.apply(tokenize)# input being English\n",
        "        df['tokens_targets'] = df.french.apply(tokenize)# target being French\n",
        "        \n",
        "    def replace_rare_tokens(self, df):\n",
        "        \"\"\"Replace rare tokens with <UNK>.\"\"\"\n",
        "        common_tokens_inputs = self.get_most_common_tokens(\n",
        "            df.tokens_inputs.tolist(),\n",
        "        )\n",
        "        common_tokens_targets = self.get_most_common_tokens(\n",
        "            df.tokens_targets.tolist(),\n",
        "        )\n",
        "        \n",
        "        df.loc[:, 'tokens_inputs'] = df.tokens_inputs.apply(\n",
        "            lambda tokens: [token if token in common_tokens_inputs \n",
        "                            else self.unknown_word_token for token in tokens]\n",
        "        )\n",
        "        df.loc[:, 'tokens_targets'] = df.tokens_targets.apply(\n",
        "            lambda tokens: [token if token in common_tokens_targets\n",
        "                            else self.unknown_word_token for token in tokens]\n",
        "        )\n",
        "\n",
        "    def get_most_common_tokens(self, tokens_series):\n",
        "        \"\"\"Return the max_vocab most common tokens.\"\"\"\n",
        "        all_tokens = self.flatten(tokens_series)\n",
        "        # Substract 4 for <PAD>, <SOS>, <EOS>, and <UNK>\n",
        "        common_tokens = set(list(zip(*Counter(all_tokens).most_common(\n",
        "            self.max_vocab - 4)))[0])\n",
        "        return common_tokens\n",
        "\n",
        "    def remove_mostly_unk(self, df, threshold=0.99):\n",
        "        \"\"\"Remove sequences with mostly <UNK>.\"\"\"\n",
        "        calculate_ratio = (\n",
        "            lambda tokens: sum(1 for token in tokens if token != '<UNK>')\n",
        "            / len(tokens) > threshold\n",
        "        )\n",
        "        df = df[df.tokens_inputs.apply(calculate_ratio)]\n",
        "        df = df[df.tokens_targets.apply(calculate_ratio)]\n",
        "        return df\n",
        "        \n",
        "    def create_token2idx(self, df):\n",
        "        \"\"\"Create variables with mappings from tokens to indices.\"\"\"\n",
        "        unique_tokens_inputs = set(self.flatten(df.tokens_inputs))\n",
        "        unique_tokens_targets = set(self.flatten(df.tokens_targets))\n",
        "        \n",
        "        for token in reversed([\n",
        "            self.padding_token,\n",
        "            self.start_of_sequence_token,\n",
        "            self.end_of_sequence_token,\n",
        "            self.unknown_word_token,\n",
        "        ]):\n",
        "            if token in unique_tokens_inputs:\n",
        "                unique_tokens_inputs.remove(token)\n",
        "            if token in unique_tokens_targets:\n",
        "                unique_tokens_targets.remove(token)\n",
        "                \n",
        "        unique_tokens_inputs = sorted(list(unique_tokens_inputs))\n",
        "        unique_tokens_targets = sorted(list(unique_tokens_targets))\n",
        "\n",
        "        # Add <PAD>, <SOS>, <EOS>, and <UNK> tokens\n",
        "        for token in reversed([\n",
        "            self.padding_token,\n",
        "            self.start_of_sequence_token,\n",
        "            self.end_of_sequence_token,\n",
        "            self.unknown_word_token,\n",
        "        ]):\n",
        "            \n",
        "            unique_tokens_inputs = [token] + unique_tokens_inputs\n",
        "            unique_tokens_targets = [token] + unique_tokens_targets\n",
        "            \n",
        "        self.token2idx_inputs = {token: idx for idx, token\n",
        "                                 in enumerate(unique_tokens_inputs)}\n",
        "        self.idx2token_inputs = {idx: token for token, idx\n",
        "                                 in self.token2idx_inputs.items()}\n",
        "        \n",
        "        self.token2idx_targets = {token: idx for idx, token\n",
        "                                  in enumerate(unique_tokens_targets)}\n",
        "        self.idx2token_targets = {idx: token for token, idx\n",
        "                                  in self.token2idx_targets.items()}\n",
        "        \n",
        "    def add_start_and_end_to_tokens(self, df):\n",
        "        \"\"\"Add <SOS> and <EOS> tokens to the end of every input and output.\"\"\"\n",
        "        df.loc[:, 'tokens_inputs'] = (\n",
        "            [self.start_of_sequence_token]\n",
        "            + df.tokens_inputs\n",
        "            + [self.end_of_sequence_token]\n",
        "        )\n",
        "        df.loc[:, 'tokens_targets'] = (\n",
        "            [self.start_of_sequence_token]\n",
        "            + df.tokens_targets\n",
        "            + [self.end_of_sequence_token]\n",
        "        )\n",
        "        \n",
        "    def tokens_to_indices(self, df):\n",
        "        \"\"\"Convert tokens to indices.\"\"\"\n",
        "        df['indices_inputs'] = df.tokens_inputs.apply(\n",
        "            lambda tokens: [self.token2idx_inputs[token] for token in tokens])\n",
        "        df['indices_targets'] = df.tokens_targets.apply(\n",
        "            lambda tokens: [self.token2idx_targets[token] for token in tokens])\n",
        "             \n",
        "        self.indices_pairs = list(zip(df.indices_inputs, df.indices_targets))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.indices_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "feHmFzzVkW_e",
        "outputId": "6bba59dc-25a0-4c3a-9585-2756df5d1fe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = EnglishFrenchTranslations(DATA_PATH, max_vocab=1000)\n",
        "len(dataset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40288"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YQ0nqfFgpOEE",
        "colab": {}
      },
      "source": [
        "train_size = int(0.999 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KvbZpy0X4DRd"
      },
      "source": [
        "### Create data generators using `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dmkANUtboAFj",
        "colab": {}
      },
      "source": [
        "def collate(batch):\n",
        "    inputs = [torch.LongTensor(item[0]) for item in batch]\n",
        "    targets = [torch.LongTensor(item[1]) for item in batch]\n",
        "    \n",
        "    # Pad sequencse so that they are all the same length (within one minibatch)\n",
        "    padded_inputs = pad_sequence(inputs, padding_value=dataset.token2idx_targets[dataset.padding_token], batch_first=True)\n",
        "    padded_targets = pad_sequence(targets, padding_value=dataset.token2idx_targets[dataset.padding_token], batch_first=True)\n",
        "    \n",
        "    # Sort by length for CUDA optimizations\n",
        "    lengths = torch.LongTensor([len(x) for x in inputs])\n",
        "    lengths, permutation = lengths.sort(dim=0, descending=True)\n",
        "\n",
        "    return padded_inputs[permutation].to(device), padded_targets[permutation].to(device), lengths.to(device)\n",
        "\n",
        "\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v4PgYxkqznDH"
      },
      "source": [
        "# 4)- Seq2Seq with Attention\n",
        "\n",
        "- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7t67To4VM0Gy"
      },
      "source": [
        "### 4a)-Define the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jWhyJzUs76_V",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            self.embedding_dim,\n",
        "            self.hidden_size,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        \n",
        "    def forward(self, inputs, lengths):\n",
        "        self.batch_size = inputs.size(0)\n",
        "        \n",
        "        # Turn input indices into distributed embeddings\n",
        "        x = self.embedding(inputs)\n",
        "\n",
        "        # Remove padding for more efficient RNN application\n",
        "        x = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "    \n",
        "        # Apply RNN to get hidden state at all timesteps (output)\n",
        "        # and hidden state of last output (self.hidden)\n",
        "        output, self.hidden = self.gru(x, self.init_hidden())\n",
        "        \n",
        "        # Pad the sequences like they were before\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Randomly initialize the weights of the RNN\n",
        "        return torch.randn(1, self.batch_size, self.hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zd9buX7BM23f"
      },
      "source": [
        "### 4b)-Define the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jX9bYm9h77vs",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size,\n",
        "        embedding_dim, \n",
        "        decoder_hidden_size,\n",
        "        encoder_hidden_size, \n",
        "        batch_size,\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.encoder_hidden_size = encoder_hidden_size\n",
        "        self.decoder_hidden_size = decoder_hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(\n",
        "            self.embedding_dim + self.encoder_hidden_size, \n",
        "            self.decoder_hidden_size,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.encoder_hidden_size, self.vocab_size)\n",
        "        \n",
        "        # Attention weights\n",
        "        self.W1 = nn.Linear(self.encoder_hidden_size, self.decoder_hidden_size)\n",
        "        self.W2 = nn.Linear(self.encoder_hidden_size, self.decoder_hidden_size)\n",
        "        self.V = nn.Linear(self.encoder_hidden_size, 1)\n",
        "    \n",
        "    def forward(self, targets, hidden, encoder_output):\n",
        "        self.batch_size = inputs.size(0)\n",
        "        \n",
        "        # Switch the dimensions of sequence_length and batch_size\n",
        "        encoder_output = encoder_output.permute(1, 0, 2)\n",
        "\n",
        "        # Add an extra axis for a time dimension\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # Attention score (Bahdanaus)\n",
        "        score = torch.tanh(self.W1(encoder_output) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "        # Attention weights\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # Find the context vectors\n",
        "        context_vector = attention_weights * encoder_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # Turn target indices into distributed embeddings\n",
        "        x = self.embedding(targets)\n",
        "        \n",
        "        # Add the context representation to the target embeddings\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # Apply the RNN\n",
        "        output, state = self.gru(x, self.init_hidden())\n",
        "        \n",
        "        # Reshape the hidden states (output)\n",
        "        output = output.view(-1, output.size(2))\n",
        "        \n",
        "        # Apply a linear layer\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def init_hidden(self):\n",
        "        # Randomly initialize the weights of the RNN\n",
        "        return torch.randn(1, self.batch_size, self.decoder_hidden_size).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Cmxk4uvM483"
      },
      "source": [
        "### 4c)- Define a model that has both an Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lDYNzCNFkYxw",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\"Calculate how wrong the model is.\"\"\"\n",
        "    # Use mask to only consider non-zero inputs in the loss\n",
        "    mask = real.ge(1).float().to(device)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)\n",
        "\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, inputs_vocab_size, targets_vocab_size, hidden_size,\n",
        "                 embedding_dim, batch_size, targets_start_idx, targets_stop_idx):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.targets_start_idx = targets_start_idx\n",
        "        self.targets_stop_idx = targets_stop_idx\n",
        "        \n",
        "        self.encoder = Encoder(inputs_vocab_size, embedding_dim,\n",
        "                               hidden_size, batch_size).to(device)\n",
        "        \n",
        "        self.decoder = Decoder(targets_vocab_size, embedding_dim,\n",
        "                               hidden_size, hidden_size, batch_size).to(device)\n",
        "        \n",
        "    def predict(self, inputs, lengths):\n",
        "        self.batch_size = inputs.size(0)\n",
        "        \n",
        "        encoder_output, encoder_hidden = self.encoder(\n",
        "            inputs.to(device),\n",
        "            lengths,\n",
        "        )\n",
        "        decoder_hidden = encoder_hidden\n",
        "    \n",
        "        # Initialize the input of the decoder to be <SOS>\n",
        "        decoder_input = torch.LongTensor(\n",
        "            [[self.targets_start_idx]] * self.batch_size,\n",
        "        )\n",
        "        \n",
        "        # Output predictions instead of loss\n",
        "        output = []\n",
        "        for _ in range(20):\n",
        "            predictions, decoder_hidden, _ = self.decoder(\n",
        "                decoder_input.to(device), \n",
        "                decoder_hidden.to(device),\n",
        "                encoder_output.to(device),\n",
        "            )\n",
        "            prediction = torch.multinomial(F.softmax(predictions, dim=1), 1)\n",
        "            decoder_input = prediction\n",
        "\n",
        "            prediction = prediction.item()\n",
        "            output.append(prediction)\n",
        "\n",
        "            if prediction == self.targets_stop_idx:\n",
        "                return output\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, inputs, targets, lengths):\n",
        "        self.batch_size = inputs.size(0)\n",
        "        \n",
        "        encoder_output, encoder_hidden = self.encoder(\n",
        "            inputs.to(device),\n",
        "            lengths,\n",
        "        )\n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        # Initialize the input of the decoder to be <SOS>\n",
        "        decoder_input = torch.LongTensor(\n",
        "            [[self.targets_start_idx]] * self.batch_size,\n",
        "        )\n",
        "                \n",
        "        # Use teacher forcing to train the model. Instead of feeding the model's\n",
        "        # own predictions to itself, feed the target token at every timestep.\n",
        "        # This leads to faster convergence\n",
        "        loss = 0\n",
        "        for timestep in range(1, targets.size(1)):\n",
        "            predictions, decoder_hidden, _ = self.decoder(\n",
        "                decoder_input.to(device), \n",
        "                decoder_hidden.to(device),\n",
        "                encoder_output.to(device),\n",
        "            )\n",
        "            decoder_input = targets[:, timestep].unsqueeze(1)\n",
        "            \n",
        "            loss += loss_function(targets[:, timestep], predictions)\n",
        "            \n",
        "        return loss / targets.size(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1fAzGv7ZMx3",
        "colab_type": "text"
      },
      "source": [
        "### 4d)- Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bea9BWHEu6-G",
        "outputId": "26f6522e-2215-4e37-c259-04060b163f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "model = EncoderDecoder(\n",
        "    inputs_vocab_size=len(dataset.token2idx_inputs),\n",
        "    targets_vocab_size=len(dataset.token2idx_targets),\n",
        "    hidden_size=256,\n",
        "    embedding_dim=100, \n",
        "    batch_size=batch_size, \n",
        "    targets_start_idx=dataset.token2idx_targets[dataset.start_of_sequence_token],\n",
        "    targets_stop_idx=dataset.token2idx_targets[dataset.end_of_sequence_token],\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(10): # no. of epochs\n",
        "    total_loss = total = 0\n",
        "    progress_bar = tqdm_notebook(train_loader, desc='Training', leave=False)\n",
        "    for inputs, targets, lengths in progress_bar:\n",
        "        # Clean old gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forwards pass\n",
        "        loss = model(inputs, targets, lengths)\n",
        "\n",
        "        # Perform gradient descent, backwards pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Take a step in the right direction\n",
        "        optimizer.step()\n",
        "\n",
        "        # Record metrics\n",
        "        total_loss += loss.item()\n",
        "        total += targets.size(1)\n",
        "\n",
        "    train_loss = total_loss / total\n",
        "    \n",
        "    tqdm.write(f'epoch #{epoch + 1:3d}\\ttrain_loss: {train_loss:.2e}\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3066f53614374807b605ba4f96047135",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  1\ttrain_loss: 8.27e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c44e7320ac84ecfb08b6f50b0454c74",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  2\ttrain_loss: 5.78e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90f65c4f564344809ffcaca5bb484a10",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  3\ttrain_loss: 4.55e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3f086f48eee4ccd81938a48a3217b60",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  4\ttrain_loss: 3.67e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fb11e1e5c28468abbb13143daa0fe4a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  5\ttrain_loss: 3.03e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50a871f7820e44e194dc2a1404e666aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  6\ttrain_loss: 2.58e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41371c6eeaf14b28a53ddaee31fa05ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  7\ttrain_loss: 2.22e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02e6bc77a6c34fc88d30397257ae046e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  8\ttrain_loss: 1.98e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0955bd16fff9470ba1440fc411c62c9d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch #  9\ttrain_loss: 1.85e-02\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ffa7147c21043c5a7facd47a2f748ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Training', max=79, style=ProgressStyle(description_width='ini…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\repoch # 10\ttrain_loss: 1.67e-02\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YIABxGEE8nhq",
        "outputId": "ad56979f-c412-4a6b-db52-ff1e747a603e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()\n",
        "total_loss = total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, _, lengths in test_loader:\n",
        "        print('>', ' '.join([\n",
        "            dataset.idx2token_inputs[idx]\n",
        "            for idx in inputs.cpu()[0].numpy()[1:-1]\n",
        "        ]))\n",
        "\n",
        "        # Forwards pass\n",
        "        outputs = model.predict(inputs, lengths)\n",
        "        print(' '.join([\n",
        "            dataset.idx2token_targets[idx]\n",
        "            for idx in outputs[:-1]\n",
        "        ]))\n",
        "        \n",
        "        print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> can he see me\n",
            "puis il me voir\n",
            "\n",
            "> tom doesn t have a job\n",
            "tom ne dispose pas s agit il\n",
            "\n",
            "> i bought the tv from her\n",
            "j ai acheté le temps de son côté\n",
            "\n",
            "> you can t wear that\n",
            "tu ne peux pas compter ça\n",
            "\n",
            "> how did you do\n",
            "vous avez fait\n",
            "\n",
            "> i m glad i was able to do this\n",
            "je me réjouis d avoir pu faire\n",
            "\n",
            "> i don t have a problem with this\n",
            "je n ai pas de problème de ce problème\n",
            "\n",
            "> can you believe this is really happening\n",
            "pouvez vous croire que ce soit là le soir\n",
            "\n",
            "> let me handle this\n",
            "laisse moi aller ceci\n",
            "\n",
            "> it is not so difficult as you think\n",
            "ce n est pas si difficile que vouloir toi\n",
            "\n",
            "> i appreciate it\n",
            "je travaille alors\n",
            "\n",
            "> she knows everything\n",
            "c est tout fait\n",
            "\n",
            "> you went to the park yesterday didn t you\n",
            "tu vas certaines vraiment toute le parc pas ce pas\n",
            "\n",
            "> it seems like an interesting job what do you exactly do\n",
            "ça a vraiment un cas place voudriez le mois faire\n",
            "\n",
            "> we re home\n",
            "nous sommes nous chez moi\n",
            "\n",
            "> come the day after tomorrow\n",
            "viens par ce là demain demain\n",
            "\n",
            "> i ve always wanted to meet tom\n",
            "ai compris seulement rencontrer tom\n",
            "\n",
            "> tom paid for it\n",
            "tom savait\n",
            "\n",
            "> do i really have to go to bed\n",
            "fais je me laisser toute chanter\n",
            "\n",
            "> i had to change the plan\n",
            "j ai dû changer de temps\n",
            "\n",
            "> do your homework by yourself\n",
            "fais d arrêter à l tellement\n",
            "\n",
            "> it was very beautiful\n",
            "c était très belle\n",
            "\n",
            "> shut the door please\n",
            "ferme par te prie\n",
            "\n",
            "> tom was late for dinner\n",
            "tom avait en retard à dîner\n",
            "\n",
            "> i wish i could see you\n",
            "j aimerais pouvoir te voir\n",
            "\n",
            "> she decided to go\n",
            "elle a décidé de partir\n",
            "\n",
            "> how long does it take to go to the office from your home\n",
            "combien de temps sait pour aller au magasin partie pour trouve au excuses au bureau la bibliothèque\n",
            "\n",
            "> it s not done yet\n",
            "ce n est pas si cassé\n",
            "\n",
            "> i thought you d left\n",
            "je pensais tu vous rendre\n",
            "\n",
            "> what did i leave behind\n",
            "à ce que je me acheter\n",
            "\n",
            "> i feel really bad about this\n",
            "je me sens vraiment mauvais\n",
            "\n",
            "> i want to cry\n",
            "je veux en train de pleurer\n",
            "\n",
            "> maybe it s not that bad\n",
            "peut être n est pas si mauvais\n",
            "\n",
            "> i didn t see you\n",
            "je ne vous ai pas vu\n",
            "\n",
            "> did you talk to tom yesterday\n",
            "avez vous parlé\n",
            "\n",
            "> we must do it quickly\n",
            "nous devons le faire rapidement\n",
            "\n",
            "> can you tell me how to get to the station\n",
            "sais tu m en laisser à toutes à la école\n",
            "\n",
            "> this makes no sense\n",
            "ça ne veut plus amusant\n",
            "\n",
            "> is this your girlfriend\n",
            "est ce que erreur de petite\n",
            "\n",
            "> are you happy right now\n",
            "es t étais en maintenant\n",
            "\n",
            "> i have no books to read\n",
            "je n ai aucun voulu me prendre\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}