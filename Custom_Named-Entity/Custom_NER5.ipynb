{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Spacy Model\n",
    "\n",
    "- Training on customized entities using Spacy's pre-trained model \n",
    "\n",
    "- Updating and adding new entities to NLPruler\n",
    "\n",
    "- Beam search algorithm for confidence score of extracted entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)- Importing key Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import EntityRuler\n",
    "import re\n",
    "import string\n",
    "import pdftotext # For pdfto text conversion\n",
    "import docx2txt # for converting docx to .txt format\n",
    "from collections import Counter\n",
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import codecs # for encoding scheme of text files\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac #  wrapper over argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from tqdm import tqdm # loading bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)-Training Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1)- Training Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title1  = [\"Agreement on Managed Data Center Services\"]\n",
    "title2  = [\"Master Services Agreement on the Provision of IT Services\"]\n",
    "title3  = [\"Master Services Agreement on the Provision of IT Services (“Agreement“ or “Master Services Agreement”)\"]\n",
    "title4  = [\"MASTER SERVICES AGREEMENT ON THE PROVISION OF MANAGED SERVICES IN PUBLIC COULDS\"]\n",
    "title5  = [\"Master Services Agreement (“Agreement“ or “Master Services Agreement”) on the provision of Managed Services in Public Clouds\"]\n",
    "title6  = [\"Agreement on the Provision of MANAGED PRINT Services\"]\n",
    "title7  = [\"Agreement on the Provision of MPS (Managed Print Services)\"]\n",
    "title8  = [\"Agreement for Security Operation Center Services\"]\n",
    "title9  = [\"AGREEMENT ON PROVISIONING OF IT AND COMMUNICATION SERVICES\"]\n",
    "title10 = [\"Agreement on Managed Data Center Services\"]\n",
    "title11 = [\"Master Project, Support and Maintenance Agreement\"]\n",
    "title12 = [\"ENTERPRISE CUSTOMER AGREEMENT\"]\n",
    "title13 = [\"AGREEMENT on the provision of managed Mobile communication Services\"]\n",
    "title14 = [\"MASTER SERVICE AGREEMENT\"]\n",
    "title15 = [\"Agreement for Security Operation Center Services\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Titles = [title2,title2, title3, title4, title5, title6, title7, title8, title9, title10, title11, title12, title13, title14,title15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA=[('Agreement on Managed Data Center Services', {'entities': [(0, len(title1[0]), 'TITLE')]})]\n",
    "start=0\n",
    "end=len(title1[0])\n",
    "for title in Titles:\n",
    "    start=end+1\n",
    "    end=start+len(title[0])\n",
    "    TRAIN_DATA.append(    (title[0], { 'entities': [(start, end , 'TITLE')]}) )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Agreement on Managed Data Center Services', {'entities': [(0, 41, 'TITLE')]}), ('Master Services Agreement on the Provision of IT Services', {'entities': [(42, 99, 'TITLE')]}), ('Master Services Agreement on the Provision of IT Services', {'entities': [(100, 157, 'TITLE')]}), ('Master Services Agreement on the Provision of IT Services (“Agreement“ or “Master Services Agreement”)', {'entities': [(158, 260, 'TITLE')]}), ('MASTER SERVICES AGREEMENT ON THE PROVISION OF MANAGED SERVICES IN PUBLIC COULDS', {'entities': [(261, 340, 'TITLE')]}), ('Master Services Agreement (“Agreement“ or “Master Services Agreement”) on the provision of Managed Services in Public Clouds', {'entities': [(341, 465, 'TITLE')]}), ('Agreement on the Provision of MANAGED PRINT Services', {'entities': [(466, 518, 'TITLE')]}), ('Agreement on the Provision of MPS (Managed Print Services)', {'entities': [(519, 577, 'TITLE')]}), ('Agreement for Security Operation Center Services', {'entities': [(578, 626, 'TITLE')]}), ('AGREEMENT ON PROVISIONING OF IT AND COMMUNICATION SERVICES', {'entities': [(627, 685, 'TITLE')]}), ('Agreement on Managed Data Center Services', {'entities': [(686, 727, 'TITLE')]}), ('Master Project, Support and Maintenance Agreement', {'entities': [(728, 777, 'TITLE')]}), ('ENTERPRISE CUSTOMER AGREEMENT', {'entities': [(778, 807, 'TITLE')]}), ('AGREEMENT on the provision of managed Mobile communication Services', {'entities': [(808, 875, 'TITLE')]}), ('MASTER SERVICE AGREEMENT', {'entities': [(876, 900, 'TITLE')]}), ('Agreement for Security Operation Center Services', {'entities': [(901, 949, 'TITLE')]})]\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training Title Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables and model path to be laoded\n",
    "model = None\n",
    "output_dir=Path(\"/Users/hassansherwani/Documents/Python/Spacy\")\n",
    "n_iter=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    }
   ],
   "source": [
    "if model is not None:\n",
    "    nlp = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity label\n",
    "LABEL = 'TITLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(model=None, new_model_name='TITLE', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for TITLE entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 65.17it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 78.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 52.435285100486}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 74.06it/s]\n",
      " 44%|████▍     | 7/16 [00:00<00:00, 69.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.0051951662490137}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 72.95it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 75.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.0000000198337875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.61it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 68.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9999964656131137}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 72.17it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 73.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.999532006123562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 72.91it/s]\n",
      " 44%|████▍     | 7/16 [00:00<00:00, 68.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.279190199220771}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 72.40it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 72.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9975813838015315}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.69it/s]\n",
      " 44%|████▍     | 7/16 [00:00<00:00, 68.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9939663740359561}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.42it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 71.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.8739948219089257}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.66it/s]\n",
      " 44%|████▍     | 7/16 [00:00<00:00, 64.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9977507801571903}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 72.79it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 76.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.0018146582995375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.22it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 70.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.002128539952392}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.14it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 74.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.0012061426803895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.47it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 67.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 11.342907059458238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.58it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 74.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.0387027841098733}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 72.74it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 73.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9480695505579733}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 73.99it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 75.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4.948707973874568}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 69.31it/s]\n",
      " 44%|████▍     | 7/16 [00:00<00:00, 67.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9953518941328796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 71.87it/s]\n",
      " 50%|█████     | 8/16 [00:00<00:00, 74.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9835424501350531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 67.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4.4135879682912345}\n",
      "Entities in 'Trained completed for TITLE entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run our Function\n",
    "extract_title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2)- Training Supplier Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppliers1 = [\"TEASYS\"]\n",
    "suppliers2 = [\"Teasys\"]\n",
    "suppliers3 = [\"TEASYS GLOBAL INVEST AG\"]\n",
    "suppliers4 = [\"Teasys Global Invest AG\"]\n",
    "suppliers5 = [\"teasys global invest ag\"]\n",
    "suppliers6 = [\"FTP\"]\n",
    "suppliers7 = [\"FTP Deutschland GmbH\"]\n",
    "suppliers8 = [\"FTP Deutschland GmbH\"]\n",
    "suppliers9 = [\"Wisniewski & Sohn GmbH\"]\n",
    "suppliers10 = [\"FBS\"]\n",
    "suppliers11 = [\"Horizon Deutschland AG\"]\n",
    "suppliers12 = [\"Horizon\"]\n",
    "suppliers13 = [\"Harpe\"]\n",
    "suppliers14 = [\"Harpe Deutschland GmbH\"]\n",
    "suppliers15 = [\"ADVENTURE SERVICES GMBH\"]\n",
    "suppliers16 = [\"Adventure Services GmbH\"]\n",
    "suppliers17 = [\"SWIPERO LIMITED\"]\n",
    "suppliers18 = [\"Swipero Limited\"]\n",
    "suppliers19 = [\"Swipero\"]\n",
    "suppliers20 = [\"Nozama Net Service\"]\n",
    "suppliers21 = [\"NOZAMA NET SERVICE\"]\n",
    "suppliers22 = [\"Schwyz Mail Solutions GmbH\"]\n",
    "suppliers23 = [\"Verizon Deutschland GmbH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppliers = [suppliers2,suppliers3,suppliers4,suppliers5,suppliers6,suppliers7,suppliers8,\n",
    "            suppliers9,suppliers10,suppliers11,suppliers12,suppliers13,suppliers14,suppliers15,suppliers16,\n",
    "            suppliers17,suppliers18,suppliers19, suppliers20,suppliers21,suppliers22,suppliers23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"TEASYS\", {'entities': [(0, len(suppliers1[0]), 'SUPPLIER')]})]\n",
    "start=0\n",
    "end=len(suppliers1[0])\n",
    "for supplier in suppliers:\n",
    "    start=end+1\n",
    "    end=start+len(supplier[0])\n",
    "    TRAIN_DATA.append(    (supplier[0], { 'entities': [(start, end , 'SUPPLIER')]}) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TEASYS', {'entities': [(0, 6, 'SUPPLIER')]}), ('Teasys', {'entities': [(7, 13, 'SUPPLIER')]}), ('TEASYS GLOBAL INVEST AG', {'entities': [(14, 37, 'SUPPLIER')]}), ('Teasys Global Invest AG', {'entities': [(38, 61, 'SUPPLIER')]}), ('teasys global invest ag', {'entities': [(62, 85, 'SUPPLIER')]}), ('FTP', {'entities': [(86, 89, 'SUPPLIER')]}), ('FTP Deutschland GmbH', {'entities': [(90, 110, 'SUPPLIER')]}), ('FTP Deutschland GmbH', {'entities': [(111, 131, 'SUPPLIER')]}), ('Wisniewski & Sohn GmbH', {'entities': [(132, 154, 'SUPPLIER')]}), ('FBS', {'entities': [(155, 158, 'SUPPLIER')]}), ('Horizon Deutschland AG', {'entities': [(159, 181, 'SUPPLIER')]}), ('Horizon', {'entities': [(182, 189, 'SUPPLIER')]}), ('Harpe', {'entities': [(190, 195, 'SUPPLIER')]}), ('Harpe Deutschland GmbH', {'entities': [(196, 218, 'SUPPLIER')]}), ('ADVENTURE SERVICES GMBH', {'entities': [(219, 242, 'SUPPLIER')]}), ('Adventure Services GmbH', {'entities': [(243, 266, 'SUPPLIER')]}), ('SWIPERO LIMITED', {'entities': [(267, 282, 'SUPPLIER')]}), ('Swipero Limited', {'entities': [(283, 298, 'SUPPLIER')]}), ('Swipero', {'entities': [(299, 306, 'SUPPLIER')]}), ('Nozama Net Service', {'entities': [(307, 325, 'SUPPLIER')]}), ('NOZAMA NET SERVICE', {'entities': [(326, 344, 'SUPPLIER')]}), ('Schwyz Mail Solutions GmbH', {'entities': [(345, 371, 'SUPPLIER')]}), ('Verizon Deutschland GmbH', {'entities': [(372, 396, 'SUPPLIER')]})]\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity label\n",
    "LABEL = 'SUPPLIER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_supplier(model=None, new_model_name='SUPPLIER', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for SUPPLIER entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 79.14it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 96.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 16.74900277562672}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 87.12it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 96.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9986699254524383}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 97.49it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 95.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.996586390935281}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 98.53it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 93.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.8307292795423964}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 95.09it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 95.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.8804470466608694}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 94.19it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 92.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.5704700659791182}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 91.14it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 97.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.7698465206648359}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 95.44it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 94.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9635555879519404}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 91.10it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 96.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.0063314292567767}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 96.35it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 94.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.37618345657481655}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 96.29it/s]\n",
      " 39%|███▉      | 9/23 [00:00<00:00, 89.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.24653992655274307}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 92.79it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 97.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.999904036521951}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 94.75it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 94.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.1735746757834584}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 94.31it/s]\n",
      " 39%|███▉      | 9/23 [00:00<00:00, 88.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.27866557828476235}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 90.86it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 97.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 9.27963235593777e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 95.07it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 96.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.7998504948530346e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 96.61it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 94.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 8.934755481667129e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 97.22it/s]\n",
      " 39%|███▉      | 9/23 [00:00<00:00, 82.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.0090262258247398e-07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 85.00it/s]\n",
      " 43%|████▎     | 10/23 [00:00<00:00, 93.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.89149047206314e-09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:00<00:00, 90.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.033997682367053444}\n",
      "Entities in 'Trained completed for SUPPLIER entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run our Function\n",
    "extract_supplier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3)- Training Client Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients1 = [\"F.UN\"]\n",
    "clients2 = [\"FUN\"]\n",
    "clients3 = [\"F.UN BUSINESS SERVICES GMBH\"]\n",
    "clients4 = [\"F.UN Business Services GmbH\"]\n",
    "clients5 = [\"F.UN Business Services Gmbh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [clients2,clients3,clients4,clients5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"F.UN\", {'entities': [(0, len(clients1[0]), 'CLIENT')]})]\n",
    "start=0\n",
    "end=len(clients1[0])\n",
    "for client in clients:\n",
    "    start=end+1\n",
    "    end=start+len(client[0])\n",
    "    TRAIN_DATA.append(    (client[0], { 'entities': [(start, end , 'CLIENT')]}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F.UN', {'entities': [(0, 4, 'CLIENT')]}), ('FUN', {'entities': [(5, 8, 'CLIENT')]}), ('F.UN BUSINESS SERVICES GMBH', {'entities': [(9, 36, 'CLIENT')]}), ('F.UN Business Services GmbH', {'entities': [(37, 64, 'CLIENT')]}), ('F.UN Business Services Gmbh', {'entities': [(65, 92, 'CLIENT')]})]\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new entity label\n",
    "LABEL = 'CLIENT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_client(model=None, new_model_name='CLIENT', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for CLIENT entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 73.63it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 92.77it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 9.033649280667305}\n",
      "{'ner': 6.479349724948406}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 87.65it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 91.89it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 3.506996516138315}\n",
      "{'ner': 2.2681233918992803}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 95.26it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 99.65it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9955286299314139}\n",
      "{'ner': 1.9916828940785098}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 98.01it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 99.41it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.999166968271468}\n",
      "{'ner': 1.9976523730154945}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 92.72it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 90.57it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.992380347663199}\n",
      "{'ner': 1.8520911290637327}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 86.81it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 95.39it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.7086376070735887}\n",
      "{'ner': 0.6766176413904098}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 98.06it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 99.52it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.832845278929554}\n",
      "{'ner': 1.8428983500782268}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 97.87it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 94.73it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.8232847003590699}\n",
      "{'ner': 0.49101646308326663}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 91.94it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 96.01it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.6402773435495825}\n",
      "{'ner': 0.1277916070847916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 96.77it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 98.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.4369164731390499}\n",
      "{'ner': 0.028647689487016934}\n",
      "Entities in 'Trained completed for CLIENT entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4)-For Dates\n",
    "\n",
    "- 1-EFFECTIVE_DATE\n",
    "- 2-Signature Date\n",
    "- 3-Termination Date\n",
    "- 4-Commencement Date\n",
    "- 5-End Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.a.EFFECTIVE_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates1 = [\"29 September 2018\"]\n",
    "dates2 = [\"01 January 2015\"]\n",
    "dates3 = [\"01.07.2018\"]\n",
    "dates4 = [\"August 2017\"]\n",
    "dates5 = [\"6 December 2016\"]\n",
    "dates6 = [\"December 2015\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_dates = [dates2,dates3,dates4,dates5,dates6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"29 September 2018\", {'entities': [(0, len(dates1[0]), 'EFFECTIVE_DATE')]})]\n",
    "start=0\n",
    "end=len(dates1[0])\n",
    "for date in eff_dates :\n",
    "    start=end+1\n",
    "    end=start+len(date[0])\n",
    "    TRAIN_DATA.append(    (date[0], { 'entities': [(start, end , 'EFFECTIVE_DATE')]}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL =\"EFFECTIVE_DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_effective_dates(model=None, new_model_name='EFFECTIVE_DATE', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for EFFECTIVE_DATE entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 78.74it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "{'ner': 10.30357140302658}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 97.54it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 100.58it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 102.37it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 103.64it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 6.269028350710869}\n",
      "{'ner': 3.4171867585973814}\n",
      "{'ner': 1.5350218202005408}\n",
      "{'ner': 1.6487036570983336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 100.07it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 99.85it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 89.85it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 95.19it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9347324424038088}\n",
      "{'ner': 1.4597308347704}\n",
      "{'ner': 1.1353474978828975}\n",
      "{'ner': 4.583332219072757}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 96.71it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 99.77it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 102.76it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 95.76it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4.496348779913544}\n",
      "{'ner': 2.3104241753891577}\n",
      "{'ner': 1.7793649710933408}\n",
      "{'ner': 1.4606332281888106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 94.03it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 97.60it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 102.22it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 99.77it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.4546013291582018}\n",
      "{'ner': 0.7485857754563525}\n",
      "{'ner': 0.32172870775165097}\n",
      "{'ner': 0.030788899752578643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 97.41it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 101.80it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 97.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.035624344387169916}\n",
      "{'ner': 0.016909284573464918}\n",
      "{'ner': 1.9079024015676046e-05}\n",
      "Entities in 'Trained completed for EFFECTIVE_DATE entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_effective_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.b.Signature Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates1 = [\"31. July 2018\"]\n",
    "dates2 = [\"August 30, 2017\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_dates = [dates2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"31. July 2018\", {'entities': [(0, len(dates1[0]), 'SIGNATURE_DATE')]})]\n",
    "start=0\n",
    "end=len(dates1[0])\n",
    "for date in sig_dates :\n",
    "    start=end+1\n",
    "    end=start+len(date[0])\n",
    "    TRAIN_DATA.append(    (date[0], { 'entities': [(start, end , 'SIGNATURE_DATE')]}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL =\"SIGNATURE_DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sign_dates(model=None, new_model_name='SIGNATURE_DATE', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for SIGNATURE_DATE entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 49.50it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 75.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 74.69it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 75.15it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "{'ner': 6.053876906633377}\n",
      "{'ner': 5.426923424005508}\n",
      "{'ner': 4.637376815080643}\n",
      "{'ner': 3.8786805272102356}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 76.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 75.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 75.43it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 78.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 79.22it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 76.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.04it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 73.44it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.7813880443573}\n",
      "{'ner': 1.496376033872366}\n",
      "{'ner': 1.7697368653607555}\n",
      "{'ner': 1.3944001489580842}\n",
      "{'ner': 1.5814265888475347}\n",
      "{'ner': 1.6313601114852645}\n",
      "{'ner': 1.6431465404888854}\n",
      "{'ner': 1.2683692755062452}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 74.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.56it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.99it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 83.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 81.92it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 82.90it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 82.15it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 82.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.490145317937131}\n",
      "{'ner': 5.865847913742545}\n",
      "{'ner': 6.331122940740414}\n",
      "{'ner': 6.162900563512863}\n",
      "{'ner': 4.929626142758082}\n",
      "{'ner': 3.335532743439229}\n",
      "{'ner': 3.450244697142516}\n",
      "{'ner': 3.1720150279145543}\n",
      "Entities in 'Trained completed for SIGNATURE_DATE entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_sign_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.c.Termination Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates1 = [\"period of 48 months\"]\n",
    "dates2 = [\"36 months\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ter_dates = [dates2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"period of 48 months\", {'entities': [(0, len(dates1[0]), 'TERMINATION_DATE')]})]\n",
    "start=0\n",
    "end=len(dates1[0])\n",
    "for date in ter_dates :\n",
    "    start=end+1\n",
    "    end=start+len(date[0])\n",
    "    TRAIN_DATA.append(    (date[0], { 'entities': [(start, end , 'TERMINATION_DATE')]}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"TERMINATION_DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_termination_dates(model=None, new_model_name='TERMINATION_DATE', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for TERMINATION_DATE entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 52.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 88.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 89.09it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 86.55it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "{'ner': 4.943156003952026}\n",
      "{'ner': 4.733199179172516}\n",
      "{'ner': 4.356341063976288}\n",
      "{'ner': 3.5204191505908966}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 82.57it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 87.85it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 90.77it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.79it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 94.12it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 90.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.02it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 85.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.38it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.9071980714797974}\n",
      "{'ner': 2.270190954208374}\n",
      "{'ner': 1.7271071132272482}\n",
      "{'ner': 1.337209813296795}\n",
      "{'ner': 1.469145882758312}\n",
      "{'ner': 1.3036364294675877}\n",
      "{'ner': 1.0354187743846524}\n",
      "{'ner': 0.9137580034630446}\n",
      "{'ner': 1.7502376503074402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 87.78it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 89.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91.30it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 93.38it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 90.62it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 89.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 6.1775904347072625}\n",
      "{'ner': 0.8103851367929167}\n",
      "{'ner': 5.661051008140973}\n",
      "{'ner': 5.383663495234032}\n",
      "{'ner': 3.7841036609124217}\n",
      "{'ner': 2.8038497413799632}\n",
      "{'ner': 3.456122545365247}\n",
      "Entities in 'Trained completed for TERMINATION_DATE entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_termination_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.d.Commencement Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates1 = [\"31.01.2017\"]\n",
    "dates2 = [\"31.03.2019\"]\n",
    "dates3 = [\"1 October 2018\"]\n",
    "dates4 = [\"September 1st, 2017\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_dates = [dates2,dates3,dates4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"31.01.2017\", {'entities': [(0, len(dates1[0]), 'COMMENCEMENT_DATE')]})]\n",
    "start=0\n",
    "end=len(dates1[0])\n",
    "for date in comm_dates :\n",
    "    start=end+1\n",
    "    end=start+len(date[0])\n",
    "    TRAIN_DATA.append(    (date[0], { 'entities': [(start, end , 'COMMENCEMENT_DATE')]}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL =\"COMMENCEMENT_DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_commencement_dates(model=None, new_model_name='COMMENCEMENT_DATE', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for Commencement Date entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 73.81it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 98.24it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "{'ner': 7.089648544788361}\n",
      "{'ner': 6.0705262422561646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 93.96it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 94.29it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 90.35it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 96.63it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 92.45it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4.51494225859642}\n",
      "{'ner': 3.0207329988479614}\n",
      "{'ner': 1.4106221608817577}\n",
      "{'ner': 1.6813012509373948}\n",
      "{'ner': 1.7543828021160834}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 94.00it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 98.63it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 94.16it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 99.85it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 98.70it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9406036896752994}\n",
      "{'ner': 1.321605007190616}\n",
      "{'ner': 1.668143688781521}\n",
      "{'ner': 0.8650690611055069}\n",
      "{'ner': 1.2267703778363552}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 96.86it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 92.98it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 91.82it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 94.44it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 93.89it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.5571290441307533}\n",
      "{'ner': 0.7114385982803557}\n",
      "{'ner': 0.4528223641294699}\n",
      "{'ner': 0.5023184886911283}\n",
      "{'ner': 0.9247863397019926}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 96.13it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 93.30it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 94.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.029219874588463278}\n",
      "{'ner': 0.007346255715742007}\n",
      "{'ner': 0.0019570532583987406}\n",
      "Entities in 'Trained completed for Commencement Date entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_commencement_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.e.End Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates1 = [\"31.12.2018\"]\n",
    "dates2 = [\"Apr 11th 2023\"]\n",
    "dates3 = [\"19.01.2020\"]\n",
    "dates4 = [\"July 31\"]\n",
    "dates5 = [\"2017\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates = [dates2,dates3,dates4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"31.12.2018\", {'entities': [(0, len(dates1[0]), 'END_DATE')]})]\n",
    "start=0\n",
    "end=len(dates1[0])\n",
    "for date in end_dates :\n",
    "    start=end+1\n",
    "    end=start+len(date[0])\n",
    "    TRAIN_DATA.append(    (date[0], { 'entities': [(start, end , 'END_DATE')]}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL =\"END_DATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_end_dates(model=None, new_model_name='END_DATE', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for END_DATE entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 69.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 95.73it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "{'ner': 4.947470456361771}\n",
      "{'ner': 4.0748313665390015}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 91.88it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 97.24it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 99.59it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 100.93it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 84.38it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 2.7247445061802864}\n",
      "{'ner': 2.191546332091093}\n",
      "{'ner': 1.4127049886155874}\n",
      "{'ner': 1.8131061284839234}\n",
      "{'ner': 1.649262026409815}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 78.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 92.55it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 93.52it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 97.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 98.48it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.918912617567969}\n",
      "{'ner': 1.5959832744164064}\n",
      "{'ner': 1.5920226904661938}\n",
      "{'ner': 1.0450129452198968}\n",
      "{'ner': 0.6247774846432943}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 95.78it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 98.19it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 100.63it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 99.07it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 101.58it/s]\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.4037211252523479}\n",
      "{'ner': 0.22260800075199222}\n",
      "{'ner': 0.11878446683609399}\n",
      "{'ner': 0.18414629697058355}\n",
      "{'ner': 0.34372974077741314}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 91.34it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 81.90it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 93.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.0012590960008085263}\n",
      "{'ner': 0.0008138548026595006}\n",
      "{'ner': 3.798874985644923e-06}\n",
      "Entities in 'Trained completed for END_DATE entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_end_dates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5)- Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries1 = [\"UK\"]\n",
    "countries2 = [\"Germany\"]\n",
    "countries3 = [\"France\"]\n",
    "countries4 = [\"Italy\"]\n",
    "countries5 = [\"Netherlands\"]\n",
    "countries6 = [\"Russia\"]\n",
    "countries7 = [\"Hungary\"]\n",
    "countries8 = [\"India\"]\n",
    "countries9 = [\"Slovakia\"]\n",
    "countries10 = [\"Czech\"]\n",
    "countries11 = [\"Australia\"]\n",
    "countries12 = [\"Vietnam\"]\n",
    "countries13 = [\"Japan\"]\n",
    "countries14 = [\"Philippines\"]\n",
    "countries15 = [\"Romania\"]\n",
    "countries16 = [\"Sweden\"]\n",
    "countries17 = [\"Czech Republic\"]\n",
    "countries18 = [\"United Kingdom\"]\n",
    "countries19 = [\"Switzerland\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "countr = [countries2,countries3,countries4,countries5,countries6,countries7,countries8,countries9,\n",
    "         countries10,countries11,countries12,countries13,countries14,countries15,countries16,countries17,\n",
    "         countries18,countries19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=[(\"UK\", {'entities': [(0, len(countries1[0]), 'COUNTRIES')]})]\n",
    "start=0\n",
    "end=len(dates1[0])\n",
    "for country in countr:\n",
    "    start=end+1\n",
    "    end=start+len(country[0])\n",
    "    TRAIN_DATA.append(    (country[0], { 'entities': [(start, end , 'COUNTRIES')]}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL =\"COUNTRIES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_countries(model=None, new_model_name='COUNTRIES', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "\n",
    "    ner.add_label(LABEL)   # add new entity label to entity recognizer\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        # Note that 'begin_training' initializes the models, so it'll zero out\n",
    "        # existing entity types.\n",
    "        optimizer = nlp.entity.create_optimizer()\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(TRAIN_DATA):\n",
    "                nlp.update([text], [annotations], sgd=optimizer, drop=0.35,\n",
    "                           losses=losses)\n",
    "            print(losses)\n",
    "\n",
    "    # Training completion, saving and ready to be loaded for future use\n",
    "    show_text = 'Trained completed for COUNTRIES entity.'\n",
    "    doc = nlp(show_text)\n",
    "    print(\"Entities in '%s'\" % show_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 9/19 [00:00<00:00, 81.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 79.07it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 102.50it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 7.701164316560607}\n",
      "{'ner': 2.0027100906128954}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 90.51it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 94.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.9981808834804895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 19/19 [00:00<00:00, 104.54it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.972686264959852}\n",
      "{'ner': 0.7604672268679362}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 85.84it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 105.15it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.5250287853463376}\n",
      "{'ner': 0.19536710097971688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 92.57it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 105.87it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.054136164835510696}\n",
      "{'ner': 0.017382937662132742}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 104.63it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 97.58it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.73794436138067e-05}\n",
      "{'ner': 1.463857707091081e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 103.54it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 99.81it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.00016493705034498386}\n",
      "{'ner': 2.7257451091900185e-10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 104.86it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 105.04it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 6.709131347594189e-09}\n",
      "{'ner': 4.740873786632132e-06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 104.18it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 106.94it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 3.036144710781991e-07}\n",
      "{'ner': 4.027426137865939e-09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 95.55it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 102.10it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 5.03091968963497e-09}\n",
      "{'ner': 7.276473127447225e-09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 103.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1.3457693911205835e-06}\n",
      "Entities in 'Trained completed for COUNTRIES entity.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_countries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)-Reassuring list of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for title\n",
    "title1  = [\"Agreement on Managed Data Center Services\"]\n",
    "title2  = [\"Master Services Agreement on the Provision of IT Services\"]\n",
    "title3  = [\"Master Services Agreement on the Provision of IT Services (“Agreement“ or “Master Services Agreement”)\"]\n",
    "title4  = [\"MASTER SERVICES AGREEMENT ON THE PROVISION OF MANAGED SERVICES IN PUBLIC COULDS\"]\n",
    "title5  = [\"Master Services Agreement (“Agreement“ or “Master Services Agreement”) on the provision of Managed Services in Public Clouds\"]\n",
    "title6  = [\"Agreement on the Provision of MANAGED PRINT Services\"]\n",
    "title7  = [\"Agreement on the Provision of MPS (Managed Print Services)\"]\n",
    "title8  = [\"Agreement for Security Operation Center Services\"]\n",
    "title9  = [\"AGREEMENT ON PROVISIONING OF IT AND COMMUNICATION SERVICES\"]\n",
    "title10 = [\"Agreement on Managed Data Center Services\"]\n",
    "title11 = [\"Master Project, Support and Maintenance Agreement\"]\n",
    "title12 = [\"ENTERPRISE CUSTOMER AGREEMENT\"]\n",
    "title13 = [\"AGREEMENT on the provision of managed Mobile communication Services\"]\n",
    "title14 = [\"MASTER SERVICE AGREEMENT\"]\n",
    "title15 = [\"Agreement for Security Operation Center Services\"]\n",
    "#for supplier\n",
    "suppliers1 = [\"TEASYS\", \"Teasys\", \"TEASYS GLOBAL INVEST AG\", \"Teasys Global Invest AG\",\"teasys global invest ag\"]\n",
    "suppliers2 = [\"FTP\", \"FTP Deutschland GmbH\", \"FTP Deutschland GmbH\"]\n",
    "suppliers3 = [\"Wisniewski & Sohn GmbH\", \"FBS\"]\n",
    "suppliers4 = [\"Horizon Deutschland AG\", \"Horizon\", \"Harpe\", \"Harpe Deutschland GmbH\"]\n",
    "suppliers5 = [\"ADVENTURE SERVICES GMBH\", \"Adventure Services GmbH\", \"SWIPERO LIMITED\", \"Swipero Limited\",\n",
    "                          \"Swipero\"]\n",
    "suppliers6 = [\"Nozama Net Service\",\"NOZAMA NET SERVICE\"]\n",
    "suppliers7 = [\"Schwyz Mail Solutions GmbH\"]\n",
    "suppliers8 = [\"Verizon Deutschland GmbH\"]\n",
    "# for client\n",
    "clients = [\"F.UN\", \"FUN\", \"F.UN BUSINESS SERVICES GMBH\", \"F.UN Business Services GmbH\"]\n",
    "#for client manager\n",
    "cli_cont_managr1 = [\"Amanda Kyzwani\"]\n",
    "#for supplier manager\n",
    "supp_cont_manar1 = [\"Tim Big\"]\n",
    "# for Dates\n",
    "dates1 = [\"29 September 2018\", \"01 January 2015\", \"01.07.2018\",\" August 2017\",\"6 December 2016\",\"December 2015\"]\n",
    "dates2 = [\"31. July 2018\",\"August 30, 2017\"]\n",
    "dates3 = [\"period of 48 months\",\"36 months\"]\n",
    "dates4 = [ \"31.01.2017\", \"31.03.2019\", \"1 October 2018\",\"September 1st, 2017\"]\n",
    "dates5 = [\"31.12.2018\", \"Apr 11th 2023\",\"19.01.2020\",\"July 31, 2017\"]\n",
    "# Countries\n",
    "countries1 = [\"UK\", \"Germany\", \"France\", \"Italy\", \"Netherlands\", \"Russia\", \"Hungary\", \"India\"]\n",
    "countries2 = [\"Slovakia\", \"Czech\", \"Australia\", \"Vietnam\", \"Japan\", \"Philippines\", \"Romania\"]\n",
    "countries3 = [\"Sweden\",\"Czech Republic\",\"United Kingdom\",\"Switzerland\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)- Buidling NLPruler using all trained entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulerAll = EntityRuler(nlp, overwrite_ents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Title entity\n",
    "\n",
    "for tit1 in title1:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit1}])\n",
    "\n",
    "for tit2 in title2:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit2}])\n",
    "\n",
    "for tit3 in title3:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit3}])\n",
    "\n",
    "for tit4 in title4:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit4}])\n",
    "\n",
    "for tit5 in title5:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit5}])\n",
    "\n",
    "for tit6 in title6:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit6}])\n",
    "\n",
    "for tit7 in title7:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit7}])\n",
    "\n",
    "for tit8 in title8:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit8}])\n",
    "    \n",
    "for tit9 in title9:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit9}])\n",
    "    \n",
    "for tit10 in title10:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit10}])\n",
    "\n",
    "for tit11 in title11:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit11}])\n",
    "\n",
    "for tit12 in title12:\n",
    "\trulerAll.add_patterns([{\"label\": \"TITLE\", \"pattern\": tit12}])\n",
    "    \n",
    "# for supplier\n",
    "\n",
    "for s1 in suppliers1:\n",
    "        rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s1}])\n",
    "\n",
    "for s2 in suppliers2:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s2}])\n",
    "\n",
    "for s3 in suppliers3:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s3}])\n",
    "\n",
    "for s4 in suppliers4:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s4}])\n",
    "\n",
    "for s5 in suppliers5:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s5}])\n",
    "    \n",
    "for s6 in suppliers6:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s6}])\n",
    "    \n",
    "for s7 in suppliers7:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s7}])\n",
    "    \n",
    "for s8 in suppliers8:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER\", \"pattern\": s8}])\n",
    "\n",
    "# for clients\n",
    "\n",
    "for c1 in clients:\n",
    "\trulerAll.add_patterns([{\"label\": \"CLIENT\", \"pattern\": c1}])\n",
    "\n",
    "# Pattern for DATES\n",
    "\n",
    "for t1 in dates1:\n",
    "\trulerAll.add_patterns([{\"label\": \"Effective-DATES\", \"pattern\": t1}])\n",
    "\n",
    "for t2 in dates2:\n",
    "\trulerAll.add_patterns([{\"label\": \"Signature-DATES\", \"pattern\": t2}])\n",
    "\n",
    "for t3 in dates3:\n",
    "\trulerAll.add_patterns([{\"label\": \"Termination-DATES\", \"pattern\": t3}])\n",
    "\n",
    "for t4 in dates4:\n",
    "\trulerAll.add_patterns([{\"label\": \"Commencement-DATES\", \"pattern\": t4}])\n",
    "\n",
    "for t5 in dates5:\n",
    "\trulerAll.add_patterns([{\"label\": \"END-DATES\", \"pattern\": t5}])\n",
    "\n",
    "# for countries\n",
    "\n",
    "for count1 in countries1:\n",
    "    rulerAll.add_patterns([{\"label\": \"COUNTRIES\", \"pattern\": count1}])\n",
    "\n",
    "for count2 in countries2:\n",
    "    rulerAll.add_patterns([{\"label\": \"COUNTRIES\", \"pattern\": count2}])\n",
    "    \n",
    "for count3 in countries3:\n",
    "    rulerAll.add_patterns([{\"label\": \"COUNTRIES\", \"pattern\": count3}])\n",
    "\n",
    "#CLIENT_CONTRACT_MANAGER\n",
    "\n",
    "for c_mangr in cli_cont_managr1:\n",
    "    rulerAll.add_patterns([{\"label\": \"CLIENT_CONTRACT_MANAGER\", \"pattern\": c_mangr}])\n",
    "\n",
    "# SUPPLIER_CONTRACT_MANAGER\n",
    "\n",
    "for supp_mangr in supp_cont_manar1:\n",
    "    rulerAll.add_patterns([{\"label\": \"SUPPLIER_CONTRACT_MANAGER\", \"pattern\": supp_mangr}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rulerAll = EntityRuler(nlp, overwrite_ents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'rulerAll']\n"
     ]
    }
   ],
   "source": [
    "rulerAll.name = 'rulerAll'\n",
    "nlp.add_pipe(rulerAll)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)-  Test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Agreement for Security Operation Center Services between Verizon Deutschland GmbH Sebrathweg 20 D-44149 Dortmund‑ hereinafter referred to as “Contractor” ‑and F.UN Business Services GmbH Humboldtstraße 33 D-30169 Hannover‑ hereinafter referred to as “EBS” - both hereinafter collectively referred to as the “Contracting Parties” ‑ October 09, 2018.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to spacy token\n",
    "with nlp.disable_pipes('ner'):\n",
    "    doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)- Confidence Score\n",
    "\n",
    "Using beam algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "scorer = Scorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<thinc.extra.search.Beam at 0x11ee845f0>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.2\n",
    "beams = nlp.entity.beam_parse([ doc ], beam_width = 3, beam_density = 0.0001)\n",
    "beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_scores = defaultdict(float)\n",
    "for beam in beams:\n",
    "    for score, ents in nlp.entity.moves.get_beam_parses(beam):\n",
    "        for start, end, label in ents:\n",
    "            entity_scores[(start, end, label)] += score\n",
    "ent_found=[]\n",
    "ent_label=[]\n",
    "ent_score=[]\n",
    "\n",
    "for key in entity_scores:\n",
    "    start, end, label = key\n",
    "    score = entity_scores[key]\n",
    "    if ( score > threshold):\n",
    "        ent_found.append(label)\n",
    "        ent_label.append(doc[start:end])\n",
    "        ent_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENT_DETECT</th>\n",
       "      <th>ENT_LABEL</th>\n",
       "      <th>CONFIDENCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TITLE</td>\n",
       "      <td>(Agreement, for, Security, Operation, Center, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUPPLIER</td>\n",
       "      <td>(Verizon, Deutschland, GmbH)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CLIENT</td>\n",
       "      <td>(F.UN, Business, Services, GmbH)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ENT_DETECT                                          ENT_LABEL  CONFIDENCE\n",
       "0      TITLE  (Agreement, for, Security, Operation, Center, ...         1.0\n",
       "1   SUPPLIER                       (Verizon, Deutschland, GmbH)         1.0\n",
       "3     CLIENT                   (F.UN, Business, Services, GmbH)         1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ent_score = pd.DataFrame({'ENT_DETECT': [], 'ENT_LABEL': [],'CONFIDENCE':[]})\n",
    "df_ent_score['ENT_DETECT']=ent_found\n",
    "df_ent_score['ENT_LABEL']=ent_label\n",
    "df_ent_score['CONFIDENCE']=ent_score\n",
    "df_ent_score[(df_ent_score.ENT_DETECT==\"TITLE\") | (df_ent_score.ENT_DETECT==\"CLIENT\") |(df_ent_score.ENT_DETECT==\"SUPPLIER\")\n",
    "             | (df_ent_score.ENT_DETECT==\"COUNTRIES\")| (df_ent_score.ENT_DETECT==\"Effective-DATES\")| (df_ent_score.ENT_DETECT==\"Signature-DATES\")\n",
    "             | (df_ent_score.ENT_DETECT==\"Termination-DATES\")| (df_ent_score.ENT_DETECT==\"Commencement-DATES\")| (df_ent_score.ENT_DETECT==\"END-DATES\")\n",
    "             | (df_ent_score.ENT_DETECT==\"CLIENT_CONTRACT_MANAGER\")| (df_ent_score.ENT_DETECT==\"SUPPLIER_CONTRACT_MANAGER\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
