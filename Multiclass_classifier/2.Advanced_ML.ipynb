{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)- Import key modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # for regular expressions \n",
    "import nltk  # for text manipulation \n",
    "import string \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import string \n",
    "\n",
    "#For Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 11, 8\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models and evaluation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier # notice its from ntlk not sklearn\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Evaluation packages\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.7.7 64bit [MSC v.1916 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "7.13.0"
        },
        {
         "module": "OS",
         "version": "Windows 10 10.0.17763 SP0"
        },
        {
         "module": "pandas",
         "version": "1.0.3"
        },
        {
         "module": "numpy",
         "version": "1.18.1"
        },
        {
         "module": "nltk",
         "version": "3.5"
        },
        {
         "module": "seaborn",
         "version": "0.10.1"
        },
        {
         "module": "matplotlib",
         "version": "3.1.3"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.7.7 64bit [MSC v.1916 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>7.13.0</td></tr><tr><td>OS</td><td>Windows 10 10.0.17763 SP0</td></tr><tr><td>pandas</td><td>1.0.3</td></tr><tr><td>numpy</td><td>1.18.1</td></tr><tr><td>nltk</td><td>3.5</td></tr><tr><td>seaborn</td><td>0.10.1</td></tr><tr><td>matplotlib</td><td>3.1.3</td></tr><tr><td colspan='2'>Fri Jun 26 17:05:33 2020 W. Europe Daylight Time</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.7.7 64bit [MSC v.1916 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 7.13.0 \\\\ \\hline\n",
       "OS & Windows 10 10.0.17763 SP0 \\\\ \\hline\n",
       "pandas & 1.0.3 \\\\ \\hline\n",
       "numpy & 1.18.1 \\\\ \\hline\n",
       "nltk & 3.5 \\\\ \\hline\n",
       "seaborn & 0.10.1 \\\\ \\hline\n",
       "matplotlib & 3.1.3 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Fri Jun 26 17:05:33 2020 W. Europe Daylight Time} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.7.7 64bit [MSC v.1916 64 bit (AMD64)]\n",
       "IPython 7.13.0\n",
       "OS Windows 10 10.0.17763 SP0\n",
       "pandas 1.0.3\n",
       "numpy 1.18.1\n",
       "nltk 3.5\n",
       "seaborn 0.10.1\n",
       "matplotlib 3.1.3\n",
       "Fri Jun 26 17:05:33 2020 W. Europe Daylight Time"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install version_information\n",
    "%reload_ext version_information\n",
    "%version_information pandas,numpy, nltk, seaborn, matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)- Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3655, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_excel('clean_3655_eng.xlsx')\n",
    "data=data.rename(columns={'Unnamed: 0':'random_columns'}) # a trick to tackle random index values\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random_columns</th>\n",
       "      <th>clean</th>\n",
       "      <th>firstmessage</th>\n",
       "      <th>dep</th>\n",
       "      <th>firstusedtextblock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>helloi tri appli voucher order receiv mail did...</td>\n",
       "      <td>Hello:&lt;br&gt;&lt;br&gt;I tried to apply a voucher to th...</td>\n",
       "      <td>Shipping issues</td>\n",
       "      <td>nichtkombiwb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wow wow wow im love acryl cover pro photo book...</td>\n",
       "      <td>WOW WOW WOW! I'm so in love with my acrylic co...</td>\n",
       "      <td>Customer feedback</td>\n",
       "      <td>feedback</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   random_columns                                              clean  \\\n",
       "0               0  helloi tri appli voucher order receiv mail did...   \n",
       "1               1  wow wow wow im love acryl cover pro photo book...   \n",
       "\n",
       "                                        firstmessage                dep  \\\n",
       "0  Hello:<br><br>I tried to apply a voucher to th...    Shipping issues   \n",
       "1  WOW WOW WOW! I'm so in love with my acrylic co...  Customer feedback   \n",
       "\n",
       "  firstusedtextblock  \n",
       "0       nichtkombiwb  \n",
       "1           feedback  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping response of Bot as target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all samples that are above 100 atleast\n",
    "#counts=data['firstusedtextblock'].value_counts()\n",
    "#df = data.loc[data['firstusedtextblock'].isin(counts.index[counts > 30])]\n",
    "#f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.firstusedtextblock.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3)- Vectorization\n",
    "\n",
    "- bag of words\n",
    "- tf-idf\n",
    "- doc2vec\n",
    "- word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling any clean values in data with other\n",
    "\n",
    "df=data.fillna('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "random_columns        0\n",
       "clean                 0\n",
       "firstmessage          0\n",
       "dep                   0\n",
       "firstusedtextblock    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3655,)\n",
      "(3655,)\n"
     ]
    }
   ],
   "source": [
    "features=df['clean']\n",
    "labels=df['dep']\n",
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1).Bag of Words\n",
    "\n",
    "Bag-of-Words is a method to represent text into numerical features.\n",
    "\n",
    "Let us understand this using a simple example. Suppose we have only 2 document\n",
    "\n",
    "- D1: He is a lazy boy. She is also lazy.\n",
    "\n",
    "- D2: Smith is a lazy person.\n",
    "\n",
    "The list created would consist of all the unique tokens in the corpus C.\n",
    "\n",
    "= [‘He’,’She’,’lazy’,’boy’,’Smith’,’person’]\n",
    "\n",
    "Here, D=2, N=6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3655, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import gensim\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# bag-of-words feature matrix\n",
    "bow = bow_vectorizer.fit_transform(df['clean'])\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2)-TF-IDF\n",
    "\n",
    "This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account not just the occurrence of a word in a single document (or tweet) but in the entire corpus.\n",
    "\n",
    "TF-IDF works by penalising the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n",
    "\n",
    "Let’s have a look at the important terms related to TF-IDF:\n",
    "\n",
    "- TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "\n",
    "- IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "\n",
    "- TF-IDF = TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3655, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['clean'])\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3)- Doc2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = df['clean'].apply(lambda x: x.split()) # tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(TaggedDocument(s, [\"clean_\" + str(i)]))\n",
    "    return output\n",
    "labeled_text = add_label(tokenized_text) # label all the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.a.Train doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = gensim.models.Doc2Vec(dm=1,dm_mean=1,vector_size=200,window=5,negative=7,min_count=5,workers=3,alpha=0.1,seed=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 3655/3655 [00:00<00:00, 3579309.16it/s]\n"
     ]
    }
   ],
   "source": [
    "model_d2v.build_vocab([i for i in tqdm(labeled_text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.b.Preparing doc2vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3655, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_text), 200))\n",
    "for i in range(len(data)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))\n",
    "\n",
    "    \n",
    "docvec_df = pd.DataFrame(docvec_arrays)\n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>-0.000980</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>-0.001539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002178</td>\n",
       "      <td>-0.001576</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.000628</td>\n",
       "      <td>-0.000629</td>\n",
       "      <td>-0.000777</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>-0.001489</td>\n",
       "      <td>-0.001988</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>-0.000381</td>\n",
       "      <td>-0.000881</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>-0.000399</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-0.001789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000939</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>-0.001158</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>-0.001806</td>\n",
       "      <td>-0.000717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002187</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>-0.002286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000846</td>\n",
       "      <td>-0.000962</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>-0.002242</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.001664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.001898</td>\n",
       "      <td>-0.001061</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>-0.002331</td>\n",
       "      <td>-0.002375</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>-0.001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002320</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.001233</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.001851</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>-0.002494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.001385</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>-0.000883</td>\n",
       "      <td>-0.000400</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>-0.000789</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.001394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.002166  0.000939 -0.000980  0.000050  0.001281  0.000061  0.001440   \n",
       "1  0.000058  0.001830  0.001868  0.002268  0.000487  0.000579 -0.001489   \n",
       "2 -0.000939  0.000681 -0.001643  0.002111 -0.001158 -0.000852  0.001565   \n",
       "3 -0.000846 -0.000962  0.001039 -0.001436 -0.000521  0.002297 -0.002230   \n",
       "4 -0.002320  0.000137  0.001443 -0.002210 -0.001233 -0.000493  0.002150   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.000500  0.000581 -0.001539  ... -0.002178 -0.001576  0.001816  0.000959   \n",
       "1 -0.001988 -0.001501  0.001159  ...  0.002264  0.002389 -0.000381 -0.000881   \n",
       "2  0.000319 -0.001806 -0.000717  ... -0.002187 -0.002190 -0.000274  0.000139   \n",
       "3 -0.002242 -0.000170 -0.001664  ...  0.001961  0.000041 -0.001898 -0.001061   \n",
       "4 -0.001851  0.001271 -0.002494  ...  0.000728  0.000380 -0.001385 -0.000499   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0 -0.000058  0.000760 -0.000628 -0.000629 -0.000777  0.001153  \n",
       "1  0.000729 -0.000314 -0.000399  0.000010  0.000101 -0.001789  \n",
       "2  0.001936  0.001624  0.002444  0.001509  0.001125 -0.002286  \n",
       "3  0.001041  0.000436 -0.002331 -0.002375 -0.001548 -0.001167  \n",
       "4 -0.000883 -0.000400  0.001900 -0.000789  0.000151  0.001394  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2167305, 2636780)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = df['clean'].apply(lambda x: x.split()) # tokenizing\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_text,\n",
    "            size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling i.e class with other types\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "model_w2v.train(tokenized_text, total_examples= len(data['clean']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08594143, -0.1393568 , -0.01676735,  0.16414452, -0.05891037,\n",
       "       -0.19802162,  0.10549298, -0.35362422,  0.03304493, -0.13664612,\n",
       "        0.29476675,  0.27462107, -0.19522385,  0.6507489 , -0.35911494,\n",
       "        0.41927707, -0.02259575, -0.02091164, -0.1625492 ,  0.17799832,\n",
       "       -0.06042916, -0.2579778 , -0.18438585,  0.4082261 , -0.18838435,\n",
       "       -0.42107424, -0.30793926,  0.28581512,  0.02010676,  0.00502999,\n",
       "        0.31703722, -0.19147067, -0.08546429, -0.03063924,  0.28291166,\n",
       "        0.08268694,  0.5034758 , -0.00929422, -0.06546594, -0.05180954,\n",
       "        0.08905365, -0.30859813, -0.37141716, -0.05231638,  0.14325944,\n",
       "       -0.12485892, -0.06324858,  0.00233839, -0.5763319 , -0.2960994 ,\n",
       "        0.23777905, -0.08020502, -0.3752152 ,  0.41769078, -0.2881874 ,\n",
       "        0.11470824,  0.44921073,  0.12638573,  0.30085662,  0.17457093,\n",
       "       -0.18340655, -0.34110963,  0.01167548,  0.13790037, -0.35857105,\n",
       "        0.6253741 , -0.55346495, -0.50293547,  0.19824928,  0.25865838,\n",
       "       -0.21376285,  0.22957623,  0.22176199,  0.19100551, -0.06869973,\n",
       "        0.40634137,  0.21390599, -0.5129517 , -0.10823833, -0.04760494,\n",
       "        0.35844022, -0.41480002,  0.04543093, -0.18581186, -0.21065545,\n",
       "        0.13383165,  0.11899269, -0.06730855, -0.20707127,  0.13030732,\n",
       "        0.09571052, -0.446115  ,  0.15244879, -0.1377724 ,  0.207832  ,\n",
       "        0.14388871, -0.13697281,  0.09141552,  0.17291899, -0.16442607,\n",
       "        0.70099944, -0.07864846,  0.52083355,  0.0946006 , -0.5152073 ,\n",
       "        0.0544125 , -0.20025095,  0.3603264 ,  0.05728675,  0.10916603,\n",
       "       -0.11254527,  0.04614629, -0.4490406 ,  0.19732232,  0.25022912,\n",
       "       -0.01368184, -0.34689286,  0.04208478, -0.33237326, -0.253739  ,\n",
       "        0.0507445 ,  0.0451348 , -0.05505246,  0.1268226 ,  0.03625335,\n",
       "        0.43518454, -0.04494169, -0.00510829, -0.23352464,  0.26704013,\n",
       "        0.30040386, -0.08873536,  0.01761666, -0.23128785,  0.2364666 ,\n",
       "       -0.39431897, -0.02797743, -0.3574861 ,  0.19756806,  0.5017869 ,\n",
       "       -0.26609704,  0.30307716,  0.3869987 ,  0.09145312,  0.07015926,\n",
       "        0.5996069 ,  0.02920015, -0.22355472, -0.2562284 , -0.03192027,\n",
       "        0.16653518, -0.27065426, -0.77281094,  0.4101013 ,  0.658741  ,\n",
       "        0.16372778, -0.02455598, -0.01679305, -0.10337414,  0.0715186 ,\n",
       "        0.31492022,  0.1481438 ,  0.04265629,  0.10617792,  0.06583012,\n",
       "        0.47631067, -0.01862544,  0.1655514 , -0.14784321,  0.16709721,\n",
       "       -0.3945343 ,  0.6346895 , -0.27938277,  0.22341548,  0.10495003,\n",
       "       -0.38567442, -0.00929594,  0.3616207 ,  0.3649262 ,  0.01017892,\n",
       "        0.10706308,  0.2511832 ,  0.1929277 , -0.27575308,  0.21234687,\n",
       "       -0.18300207,  0.373561  ,  0.15835375, -0.04986934,  0.41823542,\n",
       "       -0.079385  ,  0.00990346, -0.00283595,  0.14049974, -0.38329315,\n",
       "       -0.14919694,  0.34311298,  0.17253087,  0.10456154, -0.19259691],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv['saal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v.wv['saal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.1.Preparing Vectors for text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary           \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4.2.Preparing word2vec feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.331766</td>\n",
       "      <td>-0.038644</td>\n",
       "      <td>-0.033001</td>\n",
       "      <td>0.274877</td>\n",
       "      <td>-0.208521</td>\n",
       "      <td>-0.017309</td>\n",
       "      <td>0.086353</td>\n",
       "      <td>-0.185794</td>\n",
       "      <td>0.080092</td>\n",
       "      <td>-0.157228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076795</td>\n",
       "      <td>0.133693</td>\n",
       "      <td>-0.112057</td>\n",
       "      <td>-0.009724</td>\n",
       "      <td>-0.074638</td>\n",
       "      <td>-0.152288</td>\n",
       "      <td>0.178477</td>\n",
       "      <td>0.070667</td>\n",
       "      <td>-0.032763</td>\n",
       "      <td>0.050453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.286518</td>\n",
       "      <td>0.006342</td>\n",
       "      <td>0.102688</td>\n",
       "      <td>0.255977</td>\n",
       "      <td>-0.106073</td>\n",
       "      <td>-0.177220</td>\n",
       "      <td>-0.014828</td>\n",
       "      <td>-0.179581</td>\n",
       "      <td>0.041880</td>\n",
       "      <td>-0.187265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001617</td>\n",
       "      <td>-0.132372</td>\n",
       "      <td>-0.011269</td>\n",
       "      <td>0.196126</td>\n",
       "      <td>0.003819</td>\n",
       "      <td>-0.105417</td>\n",
       "      <td>0.204926</td>\n",
       "      <td>-0.036961</td>\n",
       "      <td>-0.013739</td>\n",
       "      <td>0.071489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.157940</td>\n",
       "      <td>0.017721</td>\n",
       "      <td>-0.114595</td>\n",
       "      <td>0.205522</td>\n",
       "      <td>-0.150483</td>\n",
       "      <td>0.072872</td>\n",
       "      <td>-0.102696</td>\n",
       "      <td>-0.037234</td>\n",
       "      <td>0.187812</td>\n",
       "      <td>-0.259148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122001</td>\n",
       "      <td>0.149522</td>\n",
       "      <td>0.092402</td>\n",
       "      <td>-0.087501</td>\n",
       "      <td>0.025010</td>\n",
       "      <td>0.150465</td>\n",
       "      <td>0.163956</td>\n",
       "      <td>-0.083834</td>\n",
       "      <td>-0.013065</td>\n",
       "      <td>0.090841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.244558</td>\n",
       "      <td>-0.087245</td>\n",
       "      <td>0.035899</td>\n",
       "      <td>0.181079</td>\n",
       "      <td>-0.178954</td>\n",
       "      <td>0.080701</td>\n",
       "      <td>-0.024710</td>\n",
       "      <td>-0.198605</td>\n",
       "      <td>0.132769</td>\n",
       "      <td>-0.009300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010740</td>\n",
       "      <td>-0.035341</td>\n",
       "      <td>0.028652</td>\n",
       "      <td>0.120433</td>\n",
       "      <td>0.127101</td>\n",
       "      <td>-0.251632</td>\n",
       "      <td>0.099583</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>0.101971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347166</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>-0.032613</td>\n",
       "      <td>0.114290</td>\n",
       "      <td>-0.086290</td>\n",
       "      <td>-0.080340</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>-0.156350</td>\n",
       "      <td>-0.106164</td>\n",
       "      <td>-0.152771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109793</td>\n",
       "      <td>0.087171</td>\n",
       "      <td>-0.220014</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>-0.008629</td>\n",
       "      <td>-0.187659</td>\n",
       "      <td>0.152867</td>\n",
       "      <td>0.060867</td>\n",
       "      <td>0.088029</td>\n",
       "      <td>0.201105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.331766 -0.038644 -0.033001  0.274877 -0.208521 -0.017309  0.086353   \n",
       "1  0.286518  0.006342  0.102688  0.255977 -0.106073 -0.177220 -0.014828   \n",
       "2  0.157940  0.017721 -0.114595  0.205522 -0.150483  0.072872 -0.102696   \n",
       "3  0.244558 -0.087245  0.035899  0.181079 -0.178954  0.080701 -0.024710   \n",
       "4  0.347166  0.011271 -0.032613  0.114290 -0.086290 -0.080340  0.003564   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0 -0.185794  0.080092 -0.157228  ...  0.076795  0.133693 -0.112057 -0.009724   \n",
       "1 -0.179581  0.041880 -0.187265  ... -0.001617 -0.132372 -0.011269  0.196126   \n",
       "2 -0.037234  0.187812 -0.259148  ...  0.122001  0.149522  0.092402 -0.087501   \n",
       "3 -0.198605  0.132769 -0.009300  ...  0.010740 -0.035341  0.028652  0.120433   \n",
       "4 -0.156350 -0.106164 -0.152771  ...  0.109793  0.087171 -0.220014 -0.001117   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0 -0.074638 -0.152288  0.178477  0.070667 -0.032763  0.050453  \n",
       "1  0.003819 -0.105417  0.204926 -0.036961 -0.013739  0.071489  \n",
       "2  0.025010  0.150465  0.163956 -0.083834 -0.013065  0.090841  \n",
       "3  0.127101 -0.251632  0.099583 -0.002252  0.014713  0.101971  \n",
       "4 -0.008629 -0.187659  0.152867  0.060867  0.088029  0.201105  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_text), 200)) \n",
    "for i in range(len(tokenized_text)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_text[i], 200)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3655, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4)-Model Building\n",
    "\n",
    "- Logistic Regression \n",
    "- Support Vector\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve,roc_auc_score,confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.a. Logistic Regression using Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3655, 1000)\n",
      "(3655,)\n"
     ]
    }
   ],
   "source": [
    "X=bow\n",
    "y=df['dep']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and validation set\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow, y,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2924, 1000)\n",
      "(731, 1000)\n",
      "(2924,)\n",
      "(731,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_bow.shape)\n",
    "print(xvalid_bow.shape)\n",
    "print(ytrain.shape)\n",
    "print(yvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# training the model\n",
    "lreg.fit(xtrain_bow, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.25114720e-03, 1.50902304e-05, 7.87329957e-06, 5.25325359e-05,\n",
       "        7.38993811e-04, 3.97275231e-05, 7.44688145e-01, 5.81113732e-06,\n",
       "        1.15586454e-01, 2.68178547e-04, 5.19969867e-02, 1.65308256e-04,\n",
       "        2.17490933e-05, 3.33104954e-05, 5.06001688e-05, 3.35933472e-06,\n",
       "        2.14868750e-05, 1.93266483e-02, 6.50283749e-02, 2.54745197e-04,\n",
       "        2.20207170e-04, 2.23270811e-04],\n",
       "       [1.15029640e-01, 4.54453468e-03, 3.22528561e-03, 1.66677191e-03,\n",
       "        1.01256907e-02, 2.39948304e-03, 7.18337242e-01, 3.35708164e-03,\n",
       "        1.11185083e-02, 1.07515490e-02, 4.98802807e-03, 1.38316589e-03,\n",
       "        2.18079872e-03, 1.01478559e-03, 1.53424966e-03, 3.58630523e-03,\n",
       "        4.50954637e-02, 8.82524788e-03, 6.72903832e-03, 1.50400977e-02,\n",
       "        2.55420351e-02, 3.52499689e-03],\n",
       "       [3.19660306e-03, 4.76074044e-04, 1.67482968e-03, 1.21733747e-03,\n",
       "        1.28329399e-02, 1.53215581e-03, 8.13601706e-01, 1.40715347e-02,\n",
       "        3.54192616e-03, 1.64081165e-03, 7.06608328e-03, 2.51980578e-03,\n",
       "        2.91518111e-03, 1.43297597e-03, 1.06163977e-03, 4.32482791e-03,\n",
       "        5.74909332e-02, 3.74279207e-02, 1.19485078e-02, 6.79640258e-04,\n",
       "        1.86570866e-02, 6.89478671e-04],\n",
       "       [3.64466728e-02, 3.69386808e-05, 1.40808723e-06, 2.89901770e-05,\n",
       "        5.85068137e-04, 5.39079917e-06, 9.15120292e-01, 3.68900034e-03,\n",
       "        8.81986723e-05, 2.48841121e-04, 6.95002308e-05, 3.17534850e-05,\n",
       "        3.51015533e-05, 1.81605219e-04, 4.00462764e-05, 2.08253456e-05,\n",
       "        1.29633461e-04, 2.16116273e-03, 4.09864234e-02, 2.45623186e-05,\n",
       "        1.76533433e-05, 5.09316816e-05],\n",
       "       [8.10855028e-03, 1.24242682e-07, 7.28151093e-10, 4.74529691e-07,\n",
       "        1.41161245e-09, 2.21737581e-08, 4.52699847e-06, 2.81492595e-08,\n",
       "        1.36676484e-04, 9.55019564e-09, 6.32237551e-04, 2.95906467e-06,\n",
       "        3.40214373e-08, 1.98554205e-07, 1.15225389e-06, 2.39238255e-05,\n",
       "        1.93253636e-09, 9.91080864e-01, 5.30983479e-06, 6.34392178e-07,\n",
       "        1.79588502e-09, 2.26793513e-06]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting on the validation set\n",
    "prediction = lreg.predict_proba(xvalid_bow)\n",
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Order management', 'Order management', 'Order management',\n",
       "       'Order management', 'Software/Webshop/App'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction over classes\n",
    "\n",
    "prediction_class=lreg.predict(xvalid_bow)\n",
    "prediction_class[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5485636114911081"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.b.Logistic Regression using TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3655, 1000)\n",
      "(3655,)\n"
     ]
    }
   ],
   "source": [
    "X=tfidf\n",
    "y=df['dep']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and validation set\n",
    "xtrain_tfidf, xvalid_tfidf, ytrain, yvalid = train_test_split(tfidf, y,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2924, 1000)\n",
      "(731, 1000)\n",
      "(2924,)\n",
      "(731,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_tfidf.shape)\n",
    "print(xvalid_tfidf.shape)\n",
    "print(ytrain.shape)\n",
    "print(yvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# training the model\n",
    "lreg.fit(xtrain_tfidf, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on the validation set\n",
    "prediction = lreg.predict_proba(xvalid_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_class=lreg.predict(xvalid_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5554035567715458"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.c. Logistic Regression using Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3655, 200)\n",
      "(3655,)\n"
     ]
    }
   ],
   "source": [
    "X=wordvec_df\n",
    "y=df['dep']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and validation set\n",
    "xtrain_word2vec, xvalid_word2vec, ytrain, yvalid = train_test_split(X, y,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2924, 200)\n",
      "(731, 200)\n",
      "(2924,)\n",
      "(731,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_word2vec.shape)\n",
    "print(xvalid_word2vec.shape)\n",
    "print(ytrain.shape)\n",
    "print(yvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "lreg.fit(xtrain_word2vec, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on the validation set\n",
    "prediction = lreg.predict_proba(xvalid_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_class=lreg.predict(xvalid_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5772913816689467"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.d. Logistic Regression using Doc2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3655, 200)\n",
      "(3655,)\n"
     ]
    }
   ],
   "source": [
    "X=docvec_df\n",
    "y=df['dep']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2924, 200)\n",
      "(731, 200)\n",
      "(2924,)\n",
      "(731,)\n"
     ]
    }
   ],
   "source": [
    "# splitting data into training and validation set\n",
    "xtrain_doc2vec, xvalid_doc2vec, ytrain, yvalid = train_test_split(X, y,random_state=42,test_size=0.2)\n",
    "print(xtrain_doc2vec.shape)\n",
    "print(xvalid_doc2vec.shape)\n",
    "print(ytrain.shape)\n",
    "print(yvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\n",
    "lreg.fit(xtrain_doc2vec, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on the validation set\n",
    "prediction = lreg.predict_proba(xvalid_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_class=lreg.predict(xvalid_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- bow=54%\n",
    "- tfidf=55%\n",
    "- word2vec=57%\n",
    "- doc2vec=41%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM using Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_bow)\n",
    "prediction_class = svc.predict(xvalid_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5471956224350205"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM using TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.560875512995896"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear',C=1, probability=True).fit(xtrain_tfidf, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_tfidf)\n",
    "prediction_class = svc.predict(xvalid_tfidf)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM using word2vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.585499316005472"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_word2vec, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_word2vec)\n",
    "prediction_class = svc.predict(xvalid_word2vec)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM using doc2vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_doc2vec, ytrain)\n",
    "prediction = svc.predict_proba(xvalid_doc2vec)\n",
    "prediction_class = svc.predict(xvalid_doc2vec)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "\n",
    "- bow = 54%\n",
    "- tfidf= 56%\n",
    "- word2vec= 58% \n",
    "- doc2vec= 41%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF with Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5704514363885089"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain)\n",
    "prediction = rf.predict_proba(xvalid_bow)\n",
    "prediction_class = rf.predict(xvalid_bow)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF with TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5677154582763337"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain)\n",
    "prediction = rf.predict_proba(xvalid_tfidf)\n",
    "prediction_class = rf.predict(xvalid_tfidf)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF with word2vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5745554035567716"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_word2vec, ytrain)\n",
    "prediction= rf.predict_proba(xvalid_word2vec)\n",
    "prediction_class = rf.predict(xvalid_word2vec)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF with doc2vec Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_doc2vec, ytrain)\n",
    "prediction= rf.predict_proba(xvalid_doc2vec)\n",
    "prediction_class = rf.predict(xvalid_doc2vec)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- bow = 57%\n",
    "- tfidf = 56%\n",
    "- word2vec = 57%\n",
    "- doc2vec = 41%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4.XGBoost\n",
    "Extreme Gradient Boosting (xgboost) is an advanced implementation of gradient boosting algorithm. It has both linear model solver and tree learning algorithms. Its ability to do parallel computation on a single machine makes it extremely fast. It also has additional features for doing cross validation and finding important variables. There are many parameters which need to be controlled to optimize the model.\n",
    "\n",
    "Some key benefits of XGBoost are:\n",
    "\n",
    "Regularization - helps in reducing overfitting\n",
    "Parallel Processing - XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "Handling Missing Values - It has an in-built routine to handle missing values.\n",
    "Built-in Cross-Validation - allows user to run a cross-validation at each iteration of the boosting process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice there is no sklearn ready made model therefore; I needed to use XGBoost from its main librrary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost using bag of words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5444596443228454"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain)\n",
    "prediction = xgb_model.predict_proba(xvalid_bow)\n",
    "prediction_class = xgb_model.predict(xvalid_bow)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost using tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5554035567715458"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain)\n",
    "prediction = xgb_model.predict_proba(xvalid_tfidf)\n",
    "prediction_class = xgb_model.predict(xvalid_tfidf)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost using word2vecfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5991792065663475"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_word2vec, ytrain)\n",
    "prediction = xgb_model.predict_proba(xvalid_word2vec)\n",
    "prediction_class = xgb_model.predict(xvalid_word2vec)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost using doc2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3679890560875513"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_doc2vec, ytrain)\n",
    "prediction = xgb_model.predict_proba(xvalid_doc2vec)\n",
    "prediction_class = xgb_model.predict(xvalid_doc2vec)\n",
    "accuracy_score(yvalid, prediction_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- bow = 54%\n",
    "- tfidf = 55%\n",
    "- word2vec = 58%\n",
    "- doc2vec = 37%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.MLPClassifier\n",
    "\n",
    "A multilayer perceptron (MLP) is a class of feedforward artificial neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP using bag of words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.518467852257182"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_bow, ytrain)\n",
    "prediction = mlp_model.predict_proba(xvalid_bow)\n",
    "prediction_class = mlp_model.predict(xvalid_bow)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP using tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5280437756497948"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_tfidf, ytrain)\n",
    "prediction = mlp_model.predict_proba(xvalid_tfidf)\n",
    "prediction_class = mlp_model.predict(xvalid_tfidf)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP using word2vecfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5595075239398085"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_word2vec, ytrain)\n",
    "prediction = mlp_model.predict_proba(xvalid_word2vec)\n",
    "prediction_class = mlp_model.predict(xvalid_word2vec)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP using doc2vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = MLPClassifier(random_state=1, max_iter=300,learning_rate_init=0.001).fit(xtrain_doc2vec, ytrain)\n",
    "prediction = mlp_model.predict_proba(xvalid_doc2vec)\n",
    "prediction_class = mlp_model.predict(xvalid_doc2vec)\n",
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- bow = 51%\n",
    "- tfidf = 52%\n",
    "- word2vec = 54%\n",
    "- doc2vec = 41%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost using word2vec gives us the best results with our given matrics i.e 58%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "xgb_model_best = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_word2vec, ytrain)\n",
    "# save model\n",
    "filename = 'xgb_model.sav'\n",
    "pickle.dump(xgb_model_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = loaded_model.predict_proba(xvalid_word2vec)\n",
    "prediction_class = loaded_model.predict(xvalid_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5991792065663475"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(yvalid, prediction_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      precision    recall  f1-score   support\n",
      "\n",
      "                                   Customer feedback       0.61      0.53      0.57        78\n",
      "                       Data protection (Datenschutz)       1.00      0.50      0.67         4\n",
      "                                   Discovery voucher       0.00      0.00      0.00         4\n",
      "                                           Marketing       0.71      0.52      0.60        23\n",
      "                                    Order management       0.65      0.83      0.73       301\n",
      "                                 Payment (Bezahlung)       0.00      0.00      0.00        12\n",
      "                                   Product (Produkt)       0.71      0.26      0.38        19\n",
      "                                   Production delays       0.00      0.00      0.00         9\n",
      "                    Professional area (Profibereich)       0.60      0.18      0.27        17\n",
      "                                   Reseller workflow       1.00      0.75      0.86         4\n",
      "                                         Rücksendung       1.00      0.14      0.25         7\n",
      "                                       ShareWithSaal       0.66      0.74      0.69        57\n",
      "                                     Shipping issues       0.75      0.19      0.30        32\n",
      "                                Software/Webshop/App       0.49      0.38      0.42        88\n",
      "                                  Special conditions       0.75      0.33      0.46         9\n",
      "   product complaints - colours (Reklamation Farben)       0.67      0.11      0.19        18\n",
      "product complaints - products (Reklamation Produkte)       0.35      0.72      0.47        47\n",
      "product complaints - software (Reklamation Software)       0.00      0.00      0.00         2\n",
      "\n",
      "                                            accuracy                           0.60       731\n",
      "                                           macro avg       0.55      0.34      0.38       731\n",
      "                                        weighted avg       0.60      0.60      0.57       731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yvalid, prediction_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "18\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(labels.nunique())\n",
    "print(yvalid.nunique())\n",
    "print(ytrain.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other values are also very consistent.\n",
    "\n",
    "- accuracy = 57.8%\n",
    "- precision = 58%\n",
    "- recall = 58%\n",
    "- f-score = 55%\n",
    "- (test samples=731)\n",
    "- No. of classes in test data = 18\n",
    "- No. of classes in train data = 22\n",
    "- Total Classes = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
